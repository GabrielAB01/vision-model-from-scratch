# vision-model-from-scratch

> **ImplÃ©mentation lÃ©gÃ¨re, en PyTorch, dâ€™un modÃ¨le Visionâ€“Langage inspirÃ© de PaliGemma.**  
> Projet rÃ©alisÃ© dans le cadre du cours **INF8225 â€“ Intelligence artificielleÂ : techniques probabilistes et dâ€™apprentissage** (Polytechnique MontrÃ©al).
> 
> **Note**Â : Ce projet est une rÃ©â€‘implÃ©mentation pÃ©dagogique, et non un modÃ¨le de production.


## ðŸ“‘ Sommaire

- [vision-model-from-scratch](#vision-model-from-scratch)
	- [ðŸ“‘ Sommaire](#-sommaire)
	- [ðŸ‘¥ Auteurs](#-auteurs)
	- [ðŸ–¼ï¸ PrÃ©sentation](#ï¸-prÃ©sentation)
	- [âœ¨ SpÃ©cifitÃ©s du projet](#-spÃ©cifitÃ©s-du-projet)
	- [ðŸ”— Poids prÃ©-entraÃ®nÃ©s](#-poids-prÃ©-entraÃ®nÃ©s)
	- [ðŸ—‚ï¸ Arborescence](#ï¸-arborescence)
	- [ðŸš€ Installation en local](#-installation-en-local)
	- [Exemples dâ€™utilisation](#exemples-dutilisation)
		- [InfÃ©rence](#infÃ©rence)
		- [Google Colab](#google-colab)
			- [**Mode dâ€™emploi rapide (Colab) :**](#mode-demploi-rapide-colab-)
	- [RÃ©fÃ©rences (voir le rapport pour plus de dÃ©tails)](#rÃ©fÃ©rences-voir-le-rapport-pour-plus-de-dÃ©tails)
	- [Remerciements](#remerciements)



## ðŸ‘¥ Auteurs

Ce projet a Ã©tÃ© rÃ©alisÃ© par **Gabriel Abenhaim** et **Camille Yverneau**, Ã©tudiants Ã  Polytechnique MontrÃ©al. 


## ðŸ–¼ï¸ PrÃ©sentation

Ce projet est une rÃ©â€‘implÃ©mentation minimaliste mais fonctionnelle dâ€™un **modÃ¨le Visionâ€“Langage**, inspirÃ© de **PaliGemma**Â :

* **Encodeur SigLIPâ€‘ViT** pour lâ€™image,
* **DÃ©codeur Gemma** pour le texte,
* Couche de **projection multimodale** partagÃ©e, le tout en moins de **3â€¯B paramÃ¨tres**.

Le but est pÃ©dagogiqueÂ : comprendre et expÃ©rimenter les briques internes dâ€™un VLM moderne, sans dÃ©pendre de bibliothÃ¨ques â€œmagiquesâ€.


## âœ¨ SpÃ©cifitÃ©s du projet

| Description |                                                                                                                                      |
| :---------: | ------------------------------------------------------------------------------------------------------------------------------------ |
|      âš™ï¸      | **100â€¯% PyTorch**Â : aucune dÃ©pendance Ã  `transformers` pour lâ€™infÃ©rence (seulement pour la rÃ©cupÃ©ration des poids, voir ci-dessous). |
|      ðŸ–¼ï¸      | **Encodeur ViTâ€‘SigLIP**Â : embeddings patchÂ + positionnels, attention multiâ€‘tÃªtes, MLP.                                               |
|      ðŸ“      | **DÃ©codeur Gemmaâ€‘2B**Â : RoPE, selfâ€‘attention, MLP.                                                                                   |
|      ðŸ”—      | **Fusion multiâ€‘modale**Â : concatÃ©nation de tokens dâ€™image & texte, masquage automatique.                                             |
|      ðŸƒ      | **Script dâ€™infÃ©rence prÃªt Ã  lâ€™emploi**Â `launch_inference.sh`Â : CPU/GPU, tÃ©lÃ©chargement des poids.                                    |


## ðŸ”— Poids prÃ©-entraÃ®nÃ©s

Le modÃ¨le **PaliGemma** n'est **pas** rÃ©-entraÃ®nÃ© localement :  
il charge directement les poids publiÃ©s sur Hugging Face [paligemma-3b-pt-224](https://huggingface.co/google/paligemma-3b-pt-224)


Pourquoi ce choix ?

* Le prÃ©-entraÃ®nement complet nÃ©cessite plusieurs centaines de GPU et des semaines de calcul.
* Les fichiers `.safetensors` officiels sont publics, vÃ©rifiÃ©s et compatibles (mÃªmes dimensions).
* Charger ces poids permet dâ€™Ã©valuer, de fine-tuner ou de dÃ©ployer le modÃ¨le sur des infrastructures plus lÃ©gÃ¨res (une carte graphique unique suffit pour lâ€™infÃ©rence et le fine-tuning).


## ðŸ—‚ï¸ Arborescence

```
vision-model-from-scratch/
â”œâ”€â”€ TP3/                # Notebook pÃ©dagogique + miniâ€‘traduction
â”œâ”€â”€ src/                # Code source principal
â”‚   â”œâ”€â”€ gemma/          # DÃ©codeur texte
â”‚   â”œâ”€â”€ paligemma/      # Pipeline visionâ€“langage (wrapper)
â”‚   â”œâ”€â”€ siglip/         # Encodeur image
â”‚   â””â”€â”€ utils/          # Fonctions partagÃ©es
â”œâ”€â”€ launch_inference.sh # Script bash (download + infÃ©rence)
â”œâ”€â”€ requirements.txt    # DÃ©pendances Python
â”œâ”€â”€ test.py             # Mini MLP jouet (exemple)
â”œâ”€â”€ test_images/        # Images dÃ©mo (hamburger, etc.)
â””â”€â”€ README.md           # Vous Ãªtes iciÂ ðŸ™‚
```



## ðŸš€ Installation en local

> **PrÃ©requis**Â : PythonÂ â‰¥Â 3.10Â ; GPUâ€¯CUDA (optionnel mais recommandÃ©).

```bash
# 1) Cloner le dÃ©pÃ´t
$ git clone https://github.com/GabrielAB01/vision-model-from-scratch.git
$ cd vision-model-from-scratch

# 2) Installer les dÃ©pendances (PyTorch, einops, pillowâ€¦)
$ pip install -r requirements.txt  # ou poetry install
```



## Exemples dâ€™utilisation

### InfÃ©rence

Le fichier `launch_inference.sh` permet de lancer lâ€™infÃ©rence sur une image donnÃ©e, avec ou sans tÃ©lÃ©chargement des poids prÃ©-entraÃ®nÃ©s.
Il est possible de spÃ©cifierÂ :
* le modÃ¨le Ã  utiliserÂ ;
* le prompt dâ€™entrÃ©e (texte)Â ;
* le chemin de lâ€™image Ã  analyserÂ ;
* le nombre de tokens max Ã  gÃ©nÃ©rer (par dÃ©fautÂ : 128).
* la tempÃ©rature de gÃ©nÃ©ration (par dÃ©fautÂ : 0.8).
* la valeur de `top_p` (par dÃ©fautÂ : 0.9).
* utiliser le GPU (CUDA) ou le CPU.

```bash
# ExÃ©cuter le script dâ€™infÃ©rence (tÃ©lÃ©chargement des poids + infÃ©rence)
$ bash launch_inference.sh
```

### Google Colab


Si vous prÃ©fÃ©rez essayer le modÃ¨le sans rien installer en local, un notebook Google Colab prÃªt Ã  lâ€™emploi est disponible iciÂ :

[![Open in Google Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/17KRVB0gxjdpaPjn9TtXmPoYq_Hq_c2fi#scrollTo=Obx4cDX1A3a0)

Il tÃ©lÃ©charge automatiquement les poids prÃ©-entraÃ®nÃ©s depuis le dÃ©pÃ´t, exÃ©cute une dÃ©monstration dâ€™infÃ©rence sur les images dâ€™exemple et propose un petit espace pour tester vos propres photos â€” le tout sur un GPU. Le fine-tuning est aussi prÃ©sentÃ© dans le notebook.

#### **Mode dâ€™emploi rapide (Colab) :**

1. **Compressez le code** depuis votre machine :

   ```bash
   zip -r project-code.zip src test_images -x "**/__pycache__/*"
   ```

2. **TÃ©lÃ©versez lâ€™archive** dans votre espace Colab (`/content/`) via le panneau *Files*.

3. **Lancez les premiÃ¨res cellules** du notebook Colab. Elles :
   - installent les dÃ©pendances Python ;
   - dÃ©compressent lâ€™archive ;
   - rÃ©cupÃ¨rent les poids prÃ©-entraÃ®nÃ©s.

4. **Saisissez votre token Hugging Face** lorsque la cellule le demande pour autoriser le tÃ©lÃ©chargement des poids.

> Vous pouvez ensuite exÃ©cuter les cellules dâ€™infÃ©rence et tester le modÃ¨le sur les images fournies ou vos propres photos, sans aucune installation locale !


## RÃ©fÃ©rences (voir le rapport pour plus de dÃ©tails)

* Beyer *etÂ al.*Â 2024 â€“ **PaliGemma**Â : *A Versatile 3â€¯B VLM for Transfer*
* Zhai *etÂ al.*Â 2023 â€“ **SigLIP**Â : *Sigmoid Loss for Languageâ€“Image Preâ€‘Training*
* Dosovitskiy *etÂ al.*Â 2020 â€“ **ViT**Â : *An Image is Worth 16x16 Words*
* Su *etÂ al.*Â 2021 â€“ **RoFormer**Â : *Rotary Position Embedding*



## Remerciements

Merci au professeur **Christopherâ€¯Pal** pour son accompagnement, Ã  **Anthonyâ€¯Gosselin** pour le support pÃ©dagogique, et aux auteurs originaux des implÃ©mentations openâ€‘source sur lesquelles ce projet sâ€™appuie.

