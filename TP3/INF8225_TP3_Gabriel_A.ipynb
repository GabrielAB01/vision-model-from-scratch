{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5tmOFPR8VmUq"
      },
      "source": [
        "# TP3, INF8225 2025, Machine translation\n",
        "\n",
        "This TP will be due on March 27th at 8:30am.\n",
        "The goal of this TP is to build a machine translation model.\n",
        "You will be comparing the performance of three different architectures:\n",
        "* A Vanilla RNN (**Implementation provided!**)\n",
        "* A GRU-RNN (done individually)\n",
        "* A Transformer (The implementation, testing and experiments with an Encoder-Decoder - done individually, but you may discuss how to do this, ideas for experiments, etc. with any of your colleagues)\n",
        "\n",
        "You are provided with the code to load and build the pytorch dataset, the implementation for the Vanilla RNN architecture\n",
        "and the code for the training loop.\n",
        "You \"only\" have to code the architectures (a GRU-RNN and a the missing parts of the Encoder-Decoder Transformer).\n",
        "Of course, the use of built-in torch layers such as `nn.GRU` or `nn.Transformer`\n",
        "is forbidden, as the TP would be much easier and you would learn much less.\n",
        "\n",
        "The source sentences are in english and the target language is french.\n",
        "\n",
        "We hope that this TP also provides you with a basic but realistic machine learning pipeline. We hope you learn a lot from the provided code.\n",
        "\n",
        "Do not forget to **select the runtime type as GPU!**\n",
        "\n",
        "**Sources**\n",
        "\n",
        "* Dataset: [Tab-delimited Bilingual Sentence Pairs](http://www.manythings.org/anki/)\n",
        "\n",
        "<!---\n",
        "M. Cettolo, C. Girardi, and M. Federico. 2012. WIT3: Web Inventory of Transcribed and Translated Talks. In Proc. of EAMT, pp. 261-268, Trento, Italy. pdf, bib. [paper](https://aclanthology.org/2012.eamt-1.60.pdf). [website](https://wit3.fbk.eu/2016-01).\n",
        "-->\n",
        "\n",
        "* The code is inspired by this [pytorch tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html).\n",
        "\n",
        "*This notebook is quite big, use the table of contents to easily navigate through it.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyCdlapMV8Hu"
      },
      "source": [
        "# Imports and data initializations\n",
        "\n",
        "We first download and parse the dataset. From the parsed sentences\n",
        "we can build the vocabularies and the torch datasets.\n",
        "The end goal of this section is to have an iterator\n",
        "that can yield the pairs of translated datasets, and\n",
        "where each sentences is made of a sequence of tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLbVbH4lu4J0"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxQLxOjRb1KY"
      },
      "outputs": [],
      "source": [
        "# Note current default torch and cuda was 2.6.0+cu124\n",
        "# We need to go back to an earlier version compatible with torchtext\n",
        "# This will generate some dependency issues (incompatible packages), but for things that we will not need for this TP\n",
        "!pip install torch==2.1.2+cu121 -f https://download.pytorch.org/whl/torch/ --force-reinstall --no-cache-dir\n",
        "!pip install torchtext==0.16.2 --force-reinstall --no-cache-dir\n",
        "!pip install numpy==1.23.5 --force-reinstall --no-cache-dir\n",
        "!pip install scikit-learn==1.1.3 --force-reinstall --no-cache-dir\n",
        "!pip install scipy==1.9.3 --force-reinstall --no-cache-dir\n",
        "!pip install spacy einops wandb torchinfo\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download fr_core_news_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJQfREvFUdoz",
        "outputId": "56f599b3-bfa6-4917-a7f6-c0e2c8554e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1.2+cu121\n",
            "DEVICE : cuda\n"
          ]
        }
      ],
      "source": [
        "from itertools import takewhile\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import torch\n",
        "# cpal\n",
        "print(torch.__version__)\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchtext\n",
        "# from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator, Vocab\n",
        "from torchtext.datasets import IWSLT2016\n",
        "import spacy\n",
        "import einops\n",
        "import wandb\n",
        "from torchinfo import summary\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"DEVICE :\", DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxNpMbkvUfGE",
        "outputId": "ba65926b-a98c-447b-d318-a38da27e0d64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-27 05:49:46--  http://www.manythings.org/anki/fra-eng.zip\n",
            "Resolving www.manythings.org (www.manythings.org)... 173.254.30.110\n",
            "Connecting to www.manythings.org (www.manythings.org)|173.254.30.110|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7943074 (7.6M) [application/zip]\n",
            "Saving to: ‘fra-eng.zip.1’\n",
            "\n",
            "fra-eng.zip.1       100%[===================>]   7.57M  4.40MB/s    in 1.7s    \n",
            "\n",
            "2025-03-27 05:49:48 (4.40 MB/s) - ‘fra-eng.zip.1’ saved [7943074/7943074]\n",
            "\n",
            "Archive:  fra-eng.zip\n",
            "replace _about.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A*\n",
            "  inflating: _about.txt              \n",
            "  inflating: fra.txt                 \n",
            "209462\n"
          ]
        }
      ],
      "source": [
        "# Our dataset\n",
        "!wget http://www.manythings.org/anki/fra-eng.zip\n",
        "!unzip fra-eng.zip\n",
        "df = pd.read_csv('fra.txt', sep='\\t', names=['english', 'french', 'attribution'])\n",
        "train = [\n",
        "    (en, fr) for en, fr in zip(df['english'], df['french'])\n",
        "]\n",
        "train, valid = train_test_split(train, test_size=0.1, random_state=0)\n",
        "print(len(train))\n",
        "en_nlp = spacy.load('en_core_web_sm')\n",
        "fr_nlp = spacy.load('fr_core_news_sm')\n",
        "def en_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in en_nlp.tokenizer(text)]\n",
        "def fr_tokenizer(text):\n",
        "    return [tok.text.lower() for tok in fr_nlp.tokenizer(text)]\n",
        "SPECIALS = ['<unk>', '<pad>', '<bos>', '<eos>']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppPj9CrnsSoW"
      },
      "source": [
        "The tokenizers are objects that are able to divide a python string into a list of tokens (words, punctuations, special tokens...) as a list of strings.\n",
        "\n",
        "The special tokens are used for a particular reasons:\n",
        "* *\\<unk\\>*: Replace an unknown word in the vocabulary by this default token\n",
        "* *\\<pad\\>*: Virtual token used to as padding token so a batch of sentences can have a unique length\n",
        "* *\\<bos\\>*: Token indicating the beggining of a sentence in the target sequence\n",
        "* *\\<eos\\>*: Token indicating the end of a sentence in the target sequence"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ddZvN5FiK9u"
      },
      "source": [
        "## Datasets\n",
        "\n",
        "Functions and classes to build the vocabularies and the torch datasets.\n",
        "The vocabulary is an object able to transform a string token into the id (an int) of that token in the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z2dKQ6PvZC_U"
      },
      "outputs": [],
      "source": [
        "class TranslationDataset(Dataset):\n",
        "    def __init__(\n",
        "            self,\n",
        "            dataset: list,\n",
        "            en_vocab: Vocab,\n",
        "            fr_vocab: Vocab,\n",
        "            en_tokenizer,\n",
        "            fr_tokenizer,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dataset = dataset\n",
        "        self.en_vocab = en_vocab\n",
        "        self.fr_vocab = fr_vocab\n",
        "        self.en_tokenizer = en_tokenizer\n",
        "        self.fr_tokenizer = fr_tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"Return the number of examples in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, index: int) -> tuple:\n",
        "        \"\"\"Return a sample.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            index: Index of the sample.\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            en_tokens: English tokens of the sample, as a LongTensor.\n",
        "            fr_tokens: French tokens of the sample, as a LongTensor.\n",
        "        \"\"\"\n",
        "        # Get the strings\n",
        "        en_sentence, fr_sentence = self.dataset[index]\n",
        "\n",
        "        # To list of words\n",
        "        # We also add the beggining-of-sentence and end-of-sentence tokens\n",
        "        en_tokens = ['<bos>'] + self.en_tokenizer(en_sentence) + ['<eos>']\n",
        "        fr_tokens = ['<bos>'] + self.fr_tokenizer(fr_sentence) + ['<eos>']\n",
        "\n",
        "        # To list of tokens\n",
        "        en_tokens = self.en_vocab(en_tokens)  # list[int]\n",
        "        fr_tokens = self.fr_vocab(fr_tokens)\n",
        "\n",
        "        return torch.LongTensor(en_tokens), torch.LongTensor(fr_tokens)\n",
        "\n",
        "\n",
        "def yield_tokens(dataset, tokenizer, lang):\n",
        "    \"\"\"Tokenize the whole dataset and yield the tokens.\n",
        "    \"\"\"\n",
        "    assert lang in ('en', 'fr')\n",
        "    sentence_idx = 0 if lang == 'en' else 1\n",
        "\n",
        "    for sentences in dataset:\n",
        "        sentence = sentences[sentence_idx]\n",
        "        tokens = tokenizer(sentence)\n",
        "        yield tokens\n",
        "\n",
        "\n",
        "def build_vocab(dataset: list, en_tokenizer, fr_tokenizer, min_freq: int):\n",
        "    \"\"\"Return two vocabularies, one for each language.\n",
        "    \"\"\"\n",
        "    en_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset, en_tokenizer, 'en'),\n",
        "        min_freq=min_freq,\n",
        "        specials=SPECIALS,\n",
        "    )\n",
        "    en_vocab.set_default_index(en_vocab['<unk>'])  # Default token for unknown words\n",
        "\n",
        "    fr_vocab = build_vocab_from_iterator(\n",
        "        yield_tokens(dataset, fr_tokenizer, 'fr'),\n",
        "        min_freq=min_freq,\n",
        "        specials=SPECIALS,\n",
        "    )\n",
        "    fr_vocab.set_default_index(fr_vocab['<unk>'])\n",
        "\n",
        "    return en_vocab, fr_vocab\n",
        "\n",
        "\n",
        "def preprocess(\n",
        "        dataset: list,\n",
        "        en_tokenizer,\n",
        "        fr_tokenizer,\n",
        "        max_words: int,\n",
        "    ) -> list:\n",
        "    \"\"\"Preprocess the dataset.\n",
        "    Remove samples where at least one of the sentences are too long.\n",
        "    Those samples takes too much memory.\n",
        "    Also remove the pending '\\n' at the end of sentences.\n",
        "    \"\"\"\n",
        "    filtered = []\n",
        "\n",
        "    for en_s, fr_s in dataset:\n",
        "        if len(en_tokenizer(en_s)) >= max_words or len(fr_tokenizer(fr_s)) >= max_words:\n",
        "            continue\n",
        "\n",
        "        en_s = en_s.replace('\\n', '')\n",
        "        fr_s = fr_s.replace('\\n', '')\n",
        "\n",
        "        filtered.append((en_s, fr_s))\n",
        "\n",
        "    return filtered\n",
        "\n",
        "\n",
        "def build_datasets(\n",
        "        max_sequence_length: int,\n",
        "        min_token_freq: int,\n",
        "        en_tokenizer,\n",
        "        fr_tokenizer,\n",
        "        train: list,\n",
        "        val: list,\n",
        "    ) -> tuple:\n",
        "    \"\"\"Build the training, validation and testing datasets.\n",
        "    It takes care of the vocabulary creation.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        - max_sequence_length: Maximum number of tokens in each sequences.\n",
        "            Having big sequences increases dramatically the VRAM taken during training.\n",
        "        - min_token_freq: Minimum number of occurences each token must have\n",
        "            to be saved in the vocabulary. Reducing this number increases\n",
        "            the vocabularies's size.\n",
        "        - en_tokenizer: Tokenizer for the english sentences.\n",
        "        - fr_tokenizer: Tokenizer for the french sentences.\n",
        "        - train and val: List containing the pairs (english, french) sentences.\n",
        "\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        - (train_dataset, val_dataset): Tuple of the two TranslationDataset objects.\n",
        "    \"\"\"\n",
        "    datasets = [\n",
        "        preprocess(samples, en_tokenizer, fr_tokenizer, max_sequence_length)\n",
        "        for samples in [train, val]\n",
        "    ]\n",
        "\n",
        "    en_vocab, fr_vocab = build_vocab(datasets[0], en_tokenizer, fr_tokenizer, min_token_freq)\n",
        "\n",
        "    datasets = [\n",
        "        TranslationDataset(samples, en_vocab, fr_vocab, en_tokenizer, fr_tokenizer)\n",
        "        for samples in datasets\n",
        "    ]\n",
        "\n",
        "    return datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GWlH-qEbkoYA"
      },
      "outputs": [],
      "source": [
        "def generate_batch(data_batch: list, src_pad_idx: int, tgt_pad_idx: int) -> tuple:\n",
        "    \"\"\"Add padding to the given batch so that all\n",
        "    the samples are of the same size.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        data_batch: List of samples.\n",
        "            Each sample is a tuple of LongTensors of varying size.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        en_batch: Batch of tokens for the padded english sentences.\n",
        "            Shape of [batch_size, max_en_len].\n",
        "        fr_batch: Batch of tokens for the padded french sentences.\n",
        "            Shape of [batch_size, max_fr_len].\n",
        "    \"\"\"\n",
        "    en_batch, fr_batch = [], []\n",
        "    for en_tokens, fr_tokens in data_batch:\n",
        "        en_batch.append(en_tokens)\n",
        "        fr_batch.append(fr_tokens)\n",
        "\n",
        "    en_batch = pad_sequence(en_batch, padding_value=src_pad_idx, batch_first=True)\n",
        "    fr_batch = pad_sequence(fr_batch, padding_value=tgt_pad_idx, batch_first=True)\n",
        "    return en_batch, fr_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wIluE6pnYry"
      },
      "source": [
        "# Models architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Gs4Myjh-jV"
      },
      "source": [
        "This is where you have to code the architectures.\n",
        "\n",
        "In a machine translation task, the model takes as input the whole\n",
        "source sentence along with the current known tokens of the target,\n",
        "and predict the next token in the target sequence.\n",
        "This means that the target tokens are predicted in an autoregressive\n",
        "manner, starting from the first token (right after the *\\<bos\\>* token) and producing tokens one by one until the last *\\<eos\\>* token.\n",
        "\n",
        "Formally, we define $s = [s_1, ..., s_{N_s}]$ as the source sequence made of $N_s$ tokens.\n",
        "We also define $t^i = [t_1, ..., t_i]$ as the target sequence at the beginning of the step $i$.\n",
        "\n",
        "The output of the model parameterized by $\\theta$ is:\n",
        "\n",
        "$$\n",
        "T_{i+1} = p(t_{i+1} | s, t^i ; \\theta )\n",
        "$$\n",
        "\n",
        "Where $T_{i+1}$ is the distribution of the next token $t_{i+1}$.\n",
        "\n",
        "The loss is simply a *cross entropy loss* over the whole steps, where each class is a token of the vocabulary.\n",
        "\n",
        "![RNN schema for machinea translation](https://www.simplilearn.com/ice9/free_resources_article_thumb/machine-translation-model-with-encoder-decoder-rnn.jpg)\n",
        "\n",
        "Note that in this image the english sentence is provided in reverse.\n",
        "\n",
        "---\n",
        "\n",
        "In pytorch, there is no dinstinction between an intermediate layer or a whole model having multiple layers in itself.\n",
        "Every layers or models inherit from the `torch.nn.Module`.\n",
        "This module needs to define the `__init__` method where you instanciate the layers,\n",
        "and the `forward` method where you decide how the inputs and the layers of the module interact between them.\n",
        "Thanks to the autograd computations of pytorch, you do not have\n",
        "to implement any backward method!\n",
        "\n",
        "A really important advice is to **always look at\n",
        "the shape of your input and your output.**\n",
        "From that, you can often guess how the layers should interact\n",
        "with the inputs to produce the right output.\n",
        "You can also easily detect if there's something wrong going on.\n",
        "\n",
        "You are more than advised to use the `einops` library and the `torch.einsum` function. This will require less operations than 'classical' code, but note that it's a bit trickier to use.\n",
        "This is a way of describing tensors manipulation with strings, bypassing the multiple tensor methods executed in the background.\n",
        "You can find a nice presentation of `einops` [here](https://einops.rocks/1-einops-basics/).\n",
        "A paper has just been released about einops [here](https://paperswithcode.com/paper/einops-clear-and-reliable-tensor).\n",
        "\n",
        "**A great tutorial on pytorch can be found [here](https://stanford.edu/class/cs224n/materials/CS224N_PyTorch_Tutorial.html).**\n",
        "Spending 3 hours on this tutorial is *no* waste of time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xodRThXg2DHM"
      },
      "source": [
        "## RNN models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvfRVUKm1u8e"
      },
      "source": [
        "### RNN\n",
        "Here, the implementation of the RNN is provided as an example. Study this code and use it as an example for the GRU implementation, if needed.\n",
        "\n",
        "The `RNNCell` layer produce one hidden state vector for each sentence in the batch\n",
        "(useful for the output of the encoder), and also produce one embedding for each\n",
        "token in each sentence (useful for the output of the decoder).\n",
        "\n",
        "The `RNN` module is composed of a stack of `RNNCell`. Each token embeddings\n",
        "coming out from a previous `RNNCell` is used as an input for the next `RNNCell` layer.\n",
        "\n",
        "**Be careful !** Our `RNNCell` implementation is not exactly the same thing as\n",
        "the PyTorch's `nn.RNNCell`. PyTorch implements only the operations for one token\n",
        "(so you would need to loop through each tokens inside the `RNN` instead).\n",
        "\n",
        "The same thing apply for the `GRU` and `GRUCell`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RiNKnwScM5Tc"
      },
      "outputs": [],
      "source": [
        "class RNNCell(nn.Module):\n",
        "    \"\"\"A single RNN layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        dropout: Dropout rate.\n",
        "\n",
        "    Important note: This layer does not exactly the same thing as nn.RNNCell does.\n",
        "    PyTorch implementation is only doing one simple pass over one token for each batch.\n",
        "    This implementation is taking the whole sequence of each batch and provide the\n",
        "    final hidden state along with the embeddings of each token in each sequence.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # See pytorch definition: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html\n",
        "        self.Wih = nn.Linear(input_size, hidden_size, device=DEVICE)\n",
        "        self.Whh = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.act = nn.Tanh()\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor) -> tuple:\n",
        "        \"\"\"Go through all the sequence in x, iteratively updating\n",
        "        the hidden state h.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Token embeddings.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Last hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, input_size = x.shape\n",
        "        y = torch.zeros([batch_size, seq_len, self.hidden_size], device=DEVICE)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "          input = x[:, t, :]\n",
        "          w_input = self.Wih(input)\n",
        "          w_hidden = self.Whh(h)\n",
        "          h = self.act(w_input + w_hidden)\n",
        "          y[:, t, :] = self.dropout(h)\n",
        "\n",
        "        return y, h\n",
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    \"\"\"Implementation of an RNN based\n",
        "    on https://pytorch.org/docs/stable/generated/torch.nn.RNN.html.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        num_layers: Number of layers (RNNCell or GRUCell).\n",
        "        dropout: Dropout rate.\n",
        "        model_type: Either 'RNN' or 'GRU', to select which model we want.\n",
        "            This parameter can be removed if you decide to use the module `GRU`.\n",
        "            Indeed, `GRU` should have exactly the same code as this module,\n",
        "            but with `GRUCell` instead of `RNNCell`. We let the freedom for you\n",
        "            to decide at which level you want to specialise the modules (either\n",
        "            in `TranslationRNN` by creating a `GRU` or a `RNN`, or in `RNN`\n",
        "            by creating a `GRUCell` or a `RNNCell`).\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            num_layers: int,\n",
        "            dropout: float,\n",
        "            model_type: str,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        model_class = RNNCell if model_type == 'RNN' else GRUCell\n",
        "\n",
        "        self.layers = nn.ModuleList()\n",
        "        self.layers.append(model_class(input_size, hidden_size, dropout))\n",
        "        for i in range(1, num_layers):\n",
        "          self.layers.append(model_class(hidden_size, hidden_size, dropout))\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor=None) -> tuple:\n",
        "        \"\"\"Pass the input sequence through all the RNN cells.\n",
        "        Returns the output and the final hidden state of each RNN layer\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Hidden state for each RNN layer.\n",
        "                Can be None, in which case an initial hidden state is created.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Output embeddings for each token after the RNN layers.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Final hidden state.\n",
        "                Shape of [batch_size, n_layers, hidden_size].\n",
        "        \"\"\"\n",
        "        input = x\n",
        "        h = torch.zeros([x.shape[0], len(self.layers), self.hidden_size], device=x.device) if h is None else h\n",
        "        final_h = torch.zeros_like(h, device=x.device)\n",
        "        for l in range(len(self.layers)):\n",
        "          input, h_out = self.layers[l](input, h[:, l, :])\n",
        "          final_h[:, l, :] = h_out\n",
        "\n",
        "        return input, final_h"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0ciaamtvK0R"
      },
      "source": [
        "### GRU\n",
        "Here you have to implement a GRU-RNN. This architecture is close to the Vanilla RNN but perform different operations. Look up the pytorch documentation to figure out the differences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdAMSZ7EMrMN"
      },
      "outputs": [],
      "source": [
        "class GRUCell(nn.Module):\n",
        "    \"\"\"A single GRU layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        input_size: Size of each input token.\n",
        "        hidden_size: Size of each RNN hidden state.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            input_size: int,\n",
        "            hidden_size: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Couches linéaires pour r_t\n",
        "        self.Wir = nn.Linear(input_size, hidden_size, device=DEVICE)\n",
        "        self.Whr = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
        "        # Couches linéaires pour z_t\n",
        "        self.Wiz = nn.Linear(input_size, hidden_size, device=DEVICE)\n",
        "        self.Whz = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
        "        # Couches linéaires pour n_t\n",
        "        self.Win = nn.Linear(input_size, hidden_size, device=DEVICE)\n",
        "        self.Whn = nn.Linear(hidden_size, hidden_size, device=DEVICE)\n",
        "\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x: torch.FloatTensor, h: torch.FloatTensor) -> tuple:\n",
        "        \"\"\"\n",
        "        Args\n",
        "        ----\n",
        "            x: Input sequence.\n",
        "                Shape of [batch_size, seq_len, input_size].\n",
        "            h: Initial hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Token embeddings.\n",
        "                Shape of [batch_size, seq_len, hidden_size].\n",
        "            h: Last hidden state.\n",
        "                Shape of [batch_size, hidden_size].\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, input_size = x.shape\n",
        "        y = torch.zeros([batch_size, seq_len, self.hidden_size], device=DEVICE)\n",
        "\n",
        "        for t in range(seq_len):\n",
        "          input = x[:, t, :]\n",
        "          r_t = self.sigmoid(self.Wir(input) + self.Whr(h))\n",
        "          z_t = self.sigmoid(self.Wiz(input) + self.Whz(h))\n",
        "          n_t = self.tanh(self.Win(input) + r_t*self.Whn(h))\n",
        "          h = (1 - z_t) * n_t + z_t * h\n",
        "          y[:, t, :] = self.dropout(h)\n",
        "        return y, h\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boIetZUy1f-5"
      },
      "source": [
        "### Translation RNN\n",
        "\n",
        "This module instanciates a vanilla RNN or a GRU-RNN and performs the translation task. This code des the following:\n",
        "* Encodes the source and target sequence\n",
        "* Passes the final hidden state of the encoder to the decoder (one for each layer)\n",
        "* Decodes the hidden state into the target sequence\n",
        "\n",
        "We use teacher forcing for training, meaning that when the next token is predicted, that prediction is based on the previous true target tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KD-6N17xhuLy"
      },
      "outputs": [],
      "source": [
        "class TranslationRNN(nn.Module):\n",
        "    \"\"\"Basic RNN encoder and decoder for a translation task.\n",
        "    It can run as a vanilla RNN or a GRU-RNN.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        n_tokens_src: Number of tokens in the source vocabulary.\n",
        "        n_tokens_tgt: Number of tokens in the target vocabulary.\n",
        "        dim_embedding: Dimension size of the word embeddings (for both language).\n",
        "        dim_hidden: Dimension size of the hidden layers in the RNNs\n",
        "            (for both the encoder and the decoder).\n",
        "        n_layers: Number of layers in the RNNs.\n",
        "        dropout: Dropout rate.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "        model_type: Either 'RNN' or 'GRU', to select which model we want.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_tokens_src: int,\n",
        "            n_tokens_tgt: int,\n",
        "            dim_embedding: int,\n",
        "            dim_hidden: int,\n",
        "            n_layers: int,\n",
        "            dropout: float,\n",
        "            src_pad_idx: int,\n",
        "            tgt_pad_idx: int,\n",
        "            model_type: str,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.src_embeddings = nn.Embedding(n_tokens_src, dim_embedding, src_pad_idx)\n",
        "        self.tgt_embeddings = nn.Embedding(n_tokens_tgt, dim_embedding, tgt_pad_idx)\n",
        "\n",
        "        self.dropout_1 = nn.Dropout(dropout)\n",
        "        self.dropout_2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder = RNN(dim_embedding, dim_hidden, n_layers, dropout, model_type)\n",
        "        self.norm = nn.LayerNorm(dim_hidden)\n",
        "        self.decoder = RNN(dim_embedding, dim_hidden, n_layers, dropout, model_type)\n",
        "        self.out_layer = nn.Linear(dim_hidden, n_tokens_tgt)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        source: torch.LongTensor,\n",
        "        target: torch.LongTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "        \"\"\"Predict the target tokens logits based on the source tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Distributions over the next token for all tokens in each sentences.\n",
        "                Those need to be the logits only, do not apply a softmax because\n",
        "                it will be done in the loss computation for numerical stability.\n",
        "                See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for more informations.\n",
        "                Shape of [batch_size, tgt_seq_len, n_tokens_tgt].\n",
        "        \"\"\"\n",
        "        source = torch.fliplr(source)\n",
        "\n",
        "        src_emb = self.src_embeddings(source)\n",
        "        out, hidden = self.encoder(src_emb)\n",
        "\n",
        "        hidden = self.norm(hidden)\n",
        "\n",
        "        tgt_emb = self.tgt_embeddings(target)\n",
        "        y, hidden = self.decoder(tgt_emb, hidden)\n",
        "\n",
        "        y = self.out_layer(y)\n",
        "\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZcGlRnZvOnY"
      },
      "source": [
        "## Transformer models\n",
        "Here you have to code the Full Transformer and Decoder-Only Transformer architectures.\n",
        "It is divided in three parts:\n",
        "* Attention layers (done individually)\n",
        "* Encoder and decoder layers (done individually)\n",
        "* Full Transformer: gather the encoder and decoder layers (done individually)\n",
        "\n",
        "The Transformer (or \"Full Transformer\") is presented in the paper: [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf). The [illustrated transformer](https://jalammar.github.io/illustrated-transformer/) blog can help you\n",
        "understanding how the architecture works.\n",
        "Once this is done, you can use [the annontated transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html) to have an idea of how to code this architecture.\n",
        "We encourage you to use `torch.einsum` and the `einops` library as much as you can. It will make your code simpler.\n",
        "\n",
        "---\n",
        "**Implementation order**\n",
        "\n",
        "To help you with the implementation, we advise you following this order:\n",
        "* Implement `TranslationTransformer` and use `nn.Transformer` instead of `Transformer`\n",
        "* Implement `Transformer` and use `nn.TransformerDecoder` and `nn.TransformerEnocder`\n",
        "* Implement the `TransformerDecoder` and `TransformerEncoder` and use `nn.MultiHeadAttention`\n",
        "* Implement `MultiHeadAttention`\n",
        "\n",
        "Do not forget to add `batch_first=True` when necessary in the `nn` modules."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwaVFTTUlYwd"
      },
      "source": [
        "### Positional Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIqHye2Vl3gk"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    This PE module comes from:\n",
        "    Pytorch. (2021). LANGUAGE MODELING WITH NN.TRANSFORMER AND TORCHTEXT. https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model: int, dropout: float, max_len: int = 5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        position = torch.arange(max_len).unsqueeze(1).to(DEVICE)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)).to(DEVICE)\n",
        "        pe = torch.zeros(max_len, 1, d_model).to(DEVICE)\n",
        "        pe[:, 0, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 0, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = einops.rearrange(x, \"b s e -> s b e\")\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Tensor, shape [seq_len, batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "        x = x + self.pe[:x.size(0)]\n",
        "        x = einops.rearrange(x, \"s b e -> b s e\")\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFxV-6M3402p"
      },
      "source": [
        "### Attention layers\n",
        "We use a `MultiHeadAttention` module, that is able to perform self-attention aswell as cross-attention (depending on what you give as queries, keys and values).\n",
        "\n",
        "**Attention**\n",
        "\n",
        "\n",
        "It takes the multiheaded queries, keys and values as input.\n",
        "It computes the attention between the queries and the keys and return the attended values.\n",
        "\n",
        "The implementation of this function can greatly be improved with *einsums*.\n",
        "\n",
        "**MultiheadAttention**\n",
        "\n",
        "Computes the multihead queries, keys and values and feed them to the `attention` function.\n",
        "You also need to merge the key padding mask and the attention mask into one mask.\n",
        "\n",
        "The implementation of this module can greatly be improved with *einops.rearrange*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0jOZxOwu_Uj"
      },
      "outputs": [],
      "source": [
        "def attention(\n",
        "        q: torch.FloatTensor,\n",
        "        k: torch.FloatTensor,\n",
        "        v: torch.FloatTensor,\n",
        "        mask: torch.BoolTensor=None,\n",
        "        dropout: nn.Dropout=None,\n",
        "    ) -> tuple:\n",
        "    \"\"\"Computes multihead scaled dot-product attention from the\n",
        "    projected queries, keys and values.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        q: Batch of queries.\n",
        "            Shape of [batch_size, seq_len_1, n_heads, dim_model].\n",
        "        k: Batch of keys.\n",
        "            Shape of [batch_size, seq_len_2, n_heads, dim_model].\n",
        "        v: Batch of values.\n",
        "            Shape of [batch_size, seq_len_2, n_heads, dim_model].\n",
        "        mask: Prevent tokens to attend to some other tokens (for padding or autoregressive attention).\n",
        "            Attention is prevented where the mask is `True`.\n",
        "            Shape of [batch_size, n_heads, seq_len_1, seq_len_2],\n",
        "            or broadcastable to that shape.\n",
        "        dropout: Dropout layer to use.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        y: Multihead scaled dot-attention between the queries, keys and values.\n",
        "            Shape of [batch_size, seq_len_1, n_heads, dim_model].\n",
        "        attn: Computed attention between the keys and the queries.\n",
        "            Shape of [batch_size, n_heads, seq_len_1, seq_len_2].\n",
        "    \"\"\"\n",
        "    qk = torch.einsum(\"b q n d , b k n d -> b n q k\", q, k) / np.sqrt(q.shape[-1]) # Normalisation par sqrt(d)\n",
        "    if mask is not None:\n",
        "        # Appliquer le masque\n",
        "        qk = qk.masked_fill(mask, -1e9)\n",
        "\n",
        "    attn = torch.softmax(qk, dim=-1)\n",
        "    if dropout is not None:\n",
        "        attn = dropout(attn)\n",
        "\n",
        "    y = torch.einsum(\"b n q k, b k n d->b q n d\", attn, v)\n",
        "\n",
        "    return y, attn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aortR1SGMLHl"
      },
      "outputs": [],
      "source": [
        "class MultiheadAttention(nn.Module):\n",
        "    \"\"\"Multihead attention module.\n",
        "    Can be used as a self-attention and cross-attention layer.\n",
        "    The queries, keys and values are projected into multiple heads\n",
        "    before computing the attention between those tensors.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        dim: Dimension of the input tokens.\n",
        "        n_heads: Number of heads. `dim` must be divisible by `n_heads`.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            dim: int,\n",
        "            n_heads: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        assert dim % n_heads == 0\n",
        "\n",
        "        self.d_model = dim\n",
        "        dk = dim // n_heads\n",
        "        self.n_heads = n_heads\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.W_q = nn.ModuleList()\n",
        "        self.W_k = nn.ModuleList()\n",
        "        self.W_v = nn.ModuleList()\n",
        "\n",
        "        for i in range(n_heads):\n",
        "            self.W_q.append(nn.Linear(dim, dk))\n",
        "            self.W_k.append(nn.Linear(dim, dk))\n",
        "            self.W_v.append(nn.Linear(dim, dk))\n",
        "\n",
        "        self.W_out = nn.Linear(dim, dim)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            q: torch.FloatTensor,\n",
        "            k: torch.FloatTensor,\n",
        "            v: torch.FloatTensor,\n",
        "            key_padding_mask: torch.BoolTensor = None,\n",
        "            attn_mask: torch.BoolTensor = None,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Computes the scaled multi-head attention form the input queries,\n",
        "        keys and values.\n",
        "\n",
        "        Project those queries, keys and values before feeding them\n",
        "        to the `attention` function.\n",
        "\n",
        "        The masks are boolean masks. Tokens are prevented to attends to\n",
        "        positions where the mask is `True`.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            q: Batch of queries.\n",
        "                Shape of [batch_size, seq_len_1, dim_model].\n",
        "            k: Batch of keys.\n",
        "                Shape of [batch_size, seq_len_2, dim_model].\n",
        "            v: Batch of values.\n",
        "                Shape of [batch_size, seq_len_2, dim_model].\n",
        "            key_padding_mask: Prevent attending to padding tokens.\n",
        "                Shape of [batch_size, seq_len_2].\n",
        "            attn_mask: Prevent attending to subsequent tokens.\n",
        "                Shape of [seq_len_1, seq_len_2].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Computed multihead attention.\n",
        "                Shape of [batch_size, seq_len_1, dim_model].\n",
        "        \"\"\"\n",
        "        # Création des masques par défaut (tous False) en cas d'absence d'un des masques\n",
        "        mask_k = einops.rearrange(\n",
        "            key_padding_mask if key_padding_mask is not None\n",
        "            else torch.zeros((k.shape[0], k.shape[1]), dtype=torch.bool, device=k.device),\n",
        "            \"b k -> b 1 1 k\"\n",
        "        )\n",
        "        mask_a = einops.rearrange(\n",
        "            attn_mask if attn_mask is not None\n",
        "            else torch.zeros((q.shape[1], k.shape[1]), dtype=torch.bool, device=q.device),\n",
        "            \"s1 s2 -> 1 1 s1 s2\"\n",
        "        )\n",
        "\n",
        "        # Expansion des masques aux dimensions attendues\n",
        "        mask_k = mask_k.expand(-1, self.n_heads, -1, -1)\n",
        "        mask_a = mask_a.expand(q.shape[0], self.n_heads, -1, -1)\n",
        "\n",
        "        # Combinaison des masques avec un OR logique\n",
        "        mask = mask_k | mask_a\n",
        "\n",
        "        # Projeter q, k, et v avec les couches linéaires W_q, W_k et W_v\n",
        "        q = torch.stack([W_q(q) for W_q in self.W_q], dim=2)\n",
        "        k = torch.stack([W_k(k) for W_k in self.W_k], dim=2)\n",
        "        v = torch.stack([W_v(v) for W_v in self.W_v], dim=2)\n",
        "\n",
        "        y, attn = attention(q, k, v, mask, self.dropout)\n",
        "\n",
        "        y = einops.rearrange(y, \"b q n d -> b q (n d)\")\n",
        "\n",
        "        y = self.W_out(y)\n",
        "\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80yEmCUUMKbB",
        "outputId": "7993332e-be7f-4f38-dcbd-9edc5828ebf2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 512])"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "att = MultiheadAttention(512, 8, 0.1).to(DEVICE)\n",
        "q = torch.randn(1, 10, 512, device=DEVICE)\n",
        "k = torch.randn(1, 20, 512, device=DEVICE)\n",
        "v = torch.randn(1, 20, 512, device=DEVICE)\n",
        "attn_mask = torch.triu(torch.ones((10, 20), dtype=torch.bool), diagonal=1).to(DEVICE)\n",
        "key_padding_mask = torch.zeros((1, 20), dtype=torch.bool).to(DEVICE)\n",
        "y = att(q, k, v, key_padding_mask, attn_mask)\n",
        "y.size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIpHjOtK47DH"
      },
      "source": [
        "### Encoder and decoder layers\n",
        "\n",
        "**TranformerEncoder**\n",
        "\n",
        "Apply self-attention layers onto the source tokens.\n",
        "It only needs the source key padding mask.\n",
        "\n",
        "\n",
        "**TranformerDecoder**\n",
        "\n",
        "Apply masked self-attention layers to the target tokens and cross-attention\n",
        "layers between the source and the target tokens.\n",
        "It needs the source and target key padding masks, and the target attention mask."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7pmiMYy4Umqv"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderLayer(nn.Module):\n",
        "    \"\"\"Single encoder layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of input tokens.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Self-attention sub-layer\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
        "\n",
        "        # Feedforward network\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "        # Normalization layers\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        src: torch.FloatTensor,\n",
        "        key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Encodes the input. Does not attend to masked inputs.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of embedded source tokens.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded source tokens.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # Self-attention sublayer\n",
        "        attn_out = self.self_attn(src, src, src, key_padding_mask=key_padding_mask)\n",
        "        src = self.norm1(src + self.dropout(attn_out))\n",
        "        feed_forward_out = self.feed_forward(src)\n",
        "        y = self.norm2(src + self.dropout(feed_forward_out))\n",
        "        return y\n",
        "\n",
        "\n",
        "class TransformerEncoder(nn.Module):\n",
        "    \"\"\"Implementation of the transformer encoder stack.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of encoders inputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        num_encoder_layers: Number of stacked encoders.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            dim_feedforward: int,\n",
        "            num_encoder_layers: int,\n",
        "            nheads: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerEncoderLayer(d_model, dim_feedforward, nheads, dropout)\n",
        "            for _ in range(num_encoder_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Encodes the source sequence by sequentially passing.\n",
        "        the source sequence through the encoder stack.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of embedded source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            key_padding_mask: Mask preventing attention to padding tokens.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Batch of encoded source sequence.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            src = layer(src, key_padding_mask)\n",
        "\n",
        "        y = self.norm(src)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d-ukpIOu_RH"
      },
      "outputs": [],
      "source": [
        "class TransformerDecoderLayer(nn.Module):\n",
        "    \"\"\"Single decoder layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of decoders inputs/outputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            nhead: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Self-attention\n",
        "        self.self_attn = MultiheadAttention(d_model, nhead, dropout)\n",
        "\n",
        "        # Cross-attention\n",
        "        self.cross_attn = MultiheadAttention(d_model, nhead, dropout)\n",
        "\n",
        "        # Feedforward network\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(d_model, d_ff),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(d_ff, d_model)\n",
        "        )\n",
        "\n",
        "        # Normalization layers\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Decode the next target tokens based on the previous tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of target sentences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y:  Batch of sequence of embeddings representing the predicted target tokens\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # Masked self-attention\n",
        "        m_attn_out = self.self_attn(\n",
        "            tgt, tgt, tgt,\n",
        "            key_padding_mask=tgt_key_padding_mask,\n",
        "            attn_mask=tgt_mask_attn\n",
        "        )\n",
        "        tgt = self.norm1(tgt + self.dropout(m_attn_out))\n",
        "\n",
        "        # Cross-attention\n",
        "        c_attn_out = self.cross_attn(\n",
        "            tgt, src, src,\n",
        "            key_padding_mask=src_key_padding_mask\n",
        "        )\n",
        "        tgt = self.norm2(tgt + self.dropout(c_attn_out))\n",
        "\n",
        "        # Feedforward\n",
        "        feed_forward_out = self.feed_forward(tgt)\n",
        "        y = self.norm3(tgt + self.dropout(feed_forward_out))\n",
        "\n",
        "        return y\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"Implementation of the transformer decoder stack.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of decoders inputs/outputs.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        num_decoder_layers: Number of stacked decoders.\n",
        "        nheads: Number of heads for each multi-head attention.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            d_ff: int,\n",
        "            num_decoder_layers:int ,\n",
        "            nhead: int,\n",
        "            dropout: float\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerDecoderLayer(d_model, d_ff, nhead, dropout)\n",
        "            for _ in range(num_decoder_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(d_model)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor,\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Decodes the source sequence by sequentially passing.\n",
        "        the encoded source sequence and the target sequence through the decoder stack.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of encoded source sentences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of taget sentences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y:  Batch of sequence of embeddings representing the predicted target tokens\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            tgt = layer(src, tgt, tgt_mask_attn, src_key_padding_mask, tgt_key_padding_mask)\n",
        "\n",
        "        y = self.norm(tgt)\n",
        "        return y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gd3kGoRO4_TV"
      },
      "source": [
        "### Transformer\n",
        "This section gathers the `Transformer` and the `TranslationTransformer` modules.\n",
        "\n",
        "**Transformer**\n",
        "\n",
        "\n",
        "The classical transformer architecture.\n",
        "It takes the source and target tokens embeddings and\n",
        "do the forward pass through the encoder and decoder.\n",
        "\n",
        "**Translation Transformer**\n",
        "\n",
        "Compute the source and target tokens embeddings, and apply a final head to produce next token logits.\n",
        "The output must not be the softmax but just the logits, because we use the `nn.CrossEntropyLoss`.\n",
        "\n",
        "It also creates the *src_key_padding_mask*, the *tgt_key_padding_mask* and the *tgt_mask_attn*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGYVF34mvRNk"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"Implementation of a Transformer based on the paper: https://arxiv.org/pdf/1706.03762.pdf.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        d_model: The dimension of encoders/decoders inputs/ouputs.\n",
        "        nhead: Number of heads for each multi-head attention.\n",
        "        num_encoder_layers: Number of stacked encoders.\n",
        "        num_decoder_layers: Number of stacked encoders.\n",
        "        dim_feedforward: Hidden dimension of the feedforward networks.\n",
        "        dropout: Dropout rate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            d_model: int,\n",
        "            nhead: int,\n",
        "            num_encoder_layers: int,\n",
        "            num_decoder_layers: int,\n",
        "            dim_feedforward: int,\n",
        "            dropout: float,\n",
        "        ):\n",
        "        super().__init__()\n",
        "        self.encoder = TransformerEncoder(d_model, dim_feedforward, num_encoder_layers, nhead, dropout)\n",
        "        self.decoder = TransformerDecoder(d_model, dim_feedforward, num_decoder_layers, nhead, dropout)\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            src: torch.FloatTensor,\n",
        "            tgt: torch.FloatTensor,\n",
        "            tgt_mask_attn: torch.BoolTensor,\n",
        "            src_key_padding_mask: torch.BoolTensor,\n",
        "            tgt_key_padding_mask: torch.BoolTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Compute next token embeddings.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            src: Batch of source sequences.\n",
        "                Shape of [batch_size, src_seq_len, dim_model].\n",
        "            tgt: Batch of target sequences.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [tgt_seq_len, tgt_seq_len].\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, src_seq_len].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, tgt_seq_len].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Next token embeddings, given the previous target tokens and the source tokens.\n",
        "                Shape of [batch_size, tgt_seq_len, dim_model].\n",
        "        \"\"\"\n",
        "        # Encoder\n",
        "        encoded_src = self.encoder(src, src_key_padding_mask)\n",
        "\n",
        "        # Decoder\n",
        "        y = self.decoder(encoded_src, tgt, tgt_mask_attn, src_key_padding_mask, tgt_key_padding_mask)\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w9WmANwGs4zP"
      },
      "outputs": [],
      "source": [
        "class TranslationTransformer(nn.Module):\n",
        "    \"\"\"Basic Transformer encoder and decoder for a translation task.\n",
        "    Manage the masks creation, and the token embeddings.\n",
        "    Position embeddings can be learnt with a standard `nn.Embedding` layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "        n_tokens_src: Number of tokens in the source vocabulary.\n",
        "        n_tokens_tgt: Number of tokens in the target vocabulary.\n",
        "        n_heads: Number of heads for each multi-head attention.\n",
        "        dim_embedding: Dimension size of the word embeddings (for both language).\n",
        "        dim_hidden: Dimension size of the feedforward layers\n",
        "            (for both the encoder and the decoder).\n",
        "        n_layers: Number of layers in the encoder and decoder.\n",
        "        dropout: Dropout rate.\n",
        "        src_pad_idx: Source padding index value.\n",
        "        tgt_pad_idx: Target padding index value.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "            self,\n",
        "            n_tokens_src: int,\n",
        "            n_tokens_tgt: int,\n",
        "            n_heads: int,\n",
        "            dim_embedding: int,\n",
        "            dim_hidden: int,\n",
        "            n_layers: int,\n",
        "            dropout: float,\n",
        "            src_pad_idx: int,\n",
        "            tgt_pad_idx: int,\n",
        "        ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Variables\n",
        "        self.src_pad_idx = src_pad_idx\n",
        "        self.tgt_pad_idx = tgt_pad_idx\n",
        "        self.dim_embedding = dim_embedding\n",
        "\n",
        "        # Embedding layers\n",
        "        self.src_embedding = nn.Embedding(n_tokens_src, dim_embedding, src_pad_idx)\n",
        "        self.tgt_embedding = nn.Embedding(n_tokens_tgt, dim_embedding, tgt_pad_idx)\n",
        "\n",
        "        # Positional encoding\n",
        "        self.pos_encoding = PositionalEncoding(dim_embedding, dropout)\n",
        "\n",
        "        self.droupout = nn.Dropout(dropout)\n",
        "\n",
        "        # Transformer\n",
        "        self.transformer = Transformer(\n",
        "            dim_embedding,\n",
        "            n_heads,\n",
        "            n_layers,\n",
        "            n_layers,\n",
        "            dim_hidden,\n",
        "            dropout\n",
        "        )\n",
        "\n",
        "        # Générateur pour générer les mots dns le vocabulaire cible\n",
        "        self.generator = nn.Linear(dim_embedding, n_tokens_tgt)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "            self,\n",
        "            source: torch.LongTensor,\n",
        "            target: torch.LongTensor\n",
        "        ) -> torch.FloatTensor:\n",
        "        \"\"\"Predict the target tokens logites based on the source tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            y: Distributions over the next token for all tokens in each sentences.\n",
        "                Those need to be the logits only, do not apply a softmax because\n",
        "                it will be done in the loss computation for numerical stability.\n",
        "                See https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html for more informations.\n",
        "                Shape of [batch_size, seq_len_tgt, n_tokens_tgt].\n",
        "        \"\"\"\n",
        "        batch_size, seq_len_src = source.shape\n",
        "        _, seq_len_tgt = target.shape\n",
        "\n",
        "        # Embedding des tokens\n",
        "        src_emb = self.src_embedding(source) # [batch_size, seq_len_src, dim_embedding]\n",
        "        tgt_emb = self.tgt_embedding(target) # [batch_size, seq_len_tgt, dim_embedding]\n",
        "\n",
        "        # Positional encoding\n",
        "        src = self.pos_encoding(src_emb)\n",
        "        tgt = self.pos_encoding(tgt_emb)\n",
        "\n",
        "        # Masks\n",
        "        tgt_mask_attn = self.generate_causal_mask(target)\n",
        "        src_key_padding_mask, tgt_key_padding_mask = self.generate_key_padding_mask(source, target)\n",
        "\n",
        "        # Transformer\n",
        "        y = self.transformer(\n",
        "            src, tgt,\n",
        "            tgt_mask_attn,\n",
        "            src_key_padding_mask, tgt_key_padding_mask\n",
        "        ) # [batch_size, seq_len_tgt, dim_embedding]\n",
        "\n",
        "        # Générateur\n",
        "        y = self.generator(y)\n",
        "\n",
        "        return y\n",
        "\n",
        "    def generate_causal_mask(\n",
        "            self,\n",
        "            target: torch.LongTensor,\n",
        "        ) -> tuple:\n",
        "        \"\"\"Generate the masks to prevent attending subsequent tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            tgt_mask_attn: Mask to prevent attention to subsequent tokens.\n",
        "                Shape of [seq_len_tgt, seq_len_tgt].\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        seq_len = target.shape[1]\n",
        "\n",
        "        tgt_mask = torch.ones((seq_len, seq_len), dtype=torch.bool)\n",
        "        tgt_mask = torch.triu(tgt_mask, diagonal=1).to(target.device)\n",
        "\n",
        "        return tgt_mask\n",
        "\n",
        "    def generate_key_padding_mask(\n",
        "            self,\n",
        "            source: torch.LongTensor,\n",
        "            target: torch.LongTensor,\n",
        "        ) -> tuple:\n",
        "        \"\"\"Generate the masks to prevent attending padding tokens.\n",
        "\n",
        "        Args\n",
        "        ----\n",
        "            source: Batch of source sentences.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            target: Batch of target sentences.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        Output\n",
        "        ------\n",
        "            src_key_padding_mask: Mask to prevent attention to padding in src sequence.\n",
        "                Shape of [batch_size, seq_len_src].\n",
        "            tgt_key_padding_mask: Mask to prevent attention to padding in tgt sequence.\n",
        "                Shape of [batch_size, seq_len_tgt].\n",
        "\n",
        "        \"\"\"\n",
        "\n",
        "        src_key_padding_mask = source == self.src_pad_idx\n",
        "        tgt_key_padding_mask = target == self.tgt_pad_idx\n",
        "\n",
        "        return src_key_padding_mask, tgt_key_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsiWkzRDkGcC"
      },
      "outputs": [],
      "source": [
        "# Test du transformer\n",
        "src = torch.randint(0, 100, (5, 25), device=DEVICE)\n",
        "tgt = torch.randint(0, 100, (5, 2), device=DEVICE)\n",
        "\n",
        "transformer = TranslationTransformer(\n",
        "    n_tokens_src=100,\n",
        "    n_tokens_tgt=100,\n",
        "    n_heads=8,\n",
        "    dim_embedding=512,\n",
        "    dim_hidden=20,\n",
        "    n_layers=6,\n",
        "    dropout=0.1,\n",
        "    src_pad_idx=0,\n",
        "    tgt_pad_idx=0,\n",
        ")\n",
        "transformer.to(DEVICE)\n",
        "y = transformer(src, tgt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzlH7LJ4oviR",
        "outputId": "e96d7ad8-5834-423b-9a44-7697643ac28f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([5, 2, 100])"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ql6jv2lAK-nF"
      },
      "source": [
        "# Greedy search\n",
        "\n",
        "One idea to explore once you have your model working is to implement a geedy search to generate a target translation from a trained model and an input source string. The next token will simply be the most probable one. Compare this strategy of decoding with the beam search strategy below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KMp7piKK905"
      },
      "outputs": [],
      "source": [
        "def greedy_search(\n",
        "        model: nn.Module,\n",
        "        source: str,\n",
        "        src_vocab: Vocab,\n",
        "        tgt_vocab: Vocab,\n",
        "        src_tokenizer,\n",
        "        device: str,\n",
        "        max_sentence_length: int,\n",
        "    ) -> str:\n",
        "    \"\"\"Do a greedy search to produce one translation.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The translation model. Assumes it produces logits score (before softmax).\n",
        "        source: The sentence to translate.\n",
        "        src_vocab: The source vocabulary.\n",
        "        tgt_vocab: The target vocabulary.\n",
        "        device: Device to which we make the inference.\n",
        "        max_sentence_length: Maximum number of tokens for the translated sentence.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        sentence: The translated source sentence.\n",
        "    \"\"\"\n",
        "    # Tokenisation de la phrase source et ajout des tokens de début et fin\n",
        "    src_tokens = ['<bos>'] + src_tokenizer(source) + ['<eos>']\n",
        "    src_indices = src_vocab(src_tokens)\n",
        "\n",
        "    # Tokenisation du token de début de la phrase cible\n",
        "    tgt_tokens = ['<bos>']\n",
        "    tgt_indices = tgt_vocab(tgt_tokens)\n",
        "\n",
        "    # To tensor and add unitary batch dimension\n",
        "    src_tensor = torch.LongTensor(src_indices).unsqueeze(dim=0).to(device) # [1, seq_len_src]\n",
        "    tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(dim=0).to(device) # [1, seq_len_tgt]\n",
        "    model.to(device)\n",
        "\n",
        "    for _ in range(max_sentence_length):\n",
        "        # Calcul des logits du modèle\n",
        "        logits = model(src_tensor,tgt_tensor)  # [1, current_tgt_len]\n",
        "        # Récupération des logits du dernier token généré\n",
        "        next_token_logits = logits[:, -1]  # [1, vocab_size]\n",
        "\n",
        "        # Sélection greedy : on sélectionne le token avec le score le plus élevé\n",
        "        next_token = torch.argmax(next_token_logits, dim=-1).item()\n",
        "        # Ajout du token dans la phrase cible\n",
        "        tgt_indices.append(next_token)\n",
        "\n",
        "        tgt_tensor = torch.LongTensor(tgt_indices).unsqueeze(0).to(device)\n",
        "\n",
        "        # Arrêt si le token <eos> est généré\n",
        "        if next_token == tgt_vocab['<eos>']:\n",
        "            break\n",
        "\n",
        "\n",
        "    # Conversion des indices en token\n",
        "    translated_tokens = tgt_vocab.lookup_tokens(tgt_indices)\n",
        "\n",
        "    # Suppression du token <bos> initial et des tokens après <eos>\n",
        "    if translated_tokens[0] == '<bos>':\n",
        "        translated_tokens = translated_tokens[1:]\n",
        "    if '<eos>' in translated_tokens:\n",
        "        translated_tokens = translated_tokens[:translated_tokens.index('<eos>')]\n",
        "\n",
        "    return \" \".join(translated_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvBtNr0lngRd"
      },
      "source": [
        "# Beam search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgGFG-uXue6w"
      },
      "source": [
        "Beam search is a smarter way of producing a sequence of tokens from\n",
        "an autoregressive model than just using a greedy search.\n",
        "\n",
        "The greedy search always chooses the most probable token as the unique\n",
        "and only next target token, and repeat this processus until the *\\<eos\\>* token is predicted.\n",
        "\n",
        "Instead, the beam search selects the k-most probable tokens at each step.\n",
        "From those k tokens, the current sequence is duplicated k times and the k tokens are appended to the k sequences to produce new k sequences.\n",
        "\n",
        "*You don't have to understand this code, but understanding this code once the TP is over could improve your torch tensors skills.*\n",
        "\n",
        "---\n",
        "\n",
        "**More explanations**\n",
        "\n",
        "Since it is done at each step, the number of sequences grows exponentially (k sequences after the first step, k² sequences after the second...).\n",
        "In order to keep the number of sequences low, we remove sequences except the top-s most likely sequences.\n",
        "To do that, we keep track of the likelihood of each sequence.\n",
        "\n",
        "Formally, we define $s = [s_1, ..., s_{N_s}]$ as the source sequence made of $N_s$ tokens.\n",
        "We also define $t^i = [t_1, ..., t_i]$ as the target sequence at the beginning of the step $i$.\n",
        "\n",
        "The output of the model parameterized by $\\theta$ is:\n",
        "\n",
        "$$\n",
        "T_{i+1} = p(t_{i+1} | s, t^i ; \\theta )\n",
        "$$\n",
        "\n",
        "Where $T_{i+1}$ is the distribution of the next token $t_{i+1}$.\n",
        "\n",
        "Then, we define the likelihood of a target sentence $t = [t_1, ..., t_{N_t}]$ as:\n",
        "\n",
        "$$\n",
        "L(t) = \\prod_{i=1}^{N_t - 1} p(t_{i+1} | s, t_{i}; \\theta )\n",
        "$$\n",
        "\n",
        "Pseudocode of the beam search:\n",
        "```\n",
        "source: [N_s source tokens]  # Shape of [total_source_tokens]\n",
        "target: [1, <bos> token]  # Shape of [n_sentences, current_target_tokens]\n",
        "target_prob: [1]  # Shape of [n_sentences]\n",
        "# We use `n_sentences` as the batch_size dimension\n",
        "\n",
        "while current_target_tokens <= max_target_length:\n",
        "    source = repeat(source, n_sentences)  # Shape of [n_sentences, total_source_tokens]\n",
        "    predicted = model(source, target)[:, -1]  # Predict the next token distributions of all the n_sentences\n",
        "    tokens_idx, tokens_prob = topk(predicted, k)\n",
        "\n",
        "    # Append the `n_sentences * k` tokens to the `n_sentences` sentences\n",
        "    target = repeat(target, k)  # Shape of [n_sentences * k, current_target_tokens]\n",
        "    target = append_tokens(target, tokens_idx)  # Shape of [n_sentences * k, current_target_tokens + 1]\n",
        "\n",
        "    # Update the sentences probabilities\n",
        "    target_prob = repeat(target_prob, k)  # Shape of [n_sentences * k]\n",
        "    target_prob *= tokens_prob\n",
        "\n",
        "    if n_sentences * k >= max_sentences:\n",
        "        target, target_prob = topk_prob(target, target_prob, k=max_sentences)\n",
        "    else:\n",
        "        n_sentences *= k\n",
        "\n",
        "    current_target_tokens += 1\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-GomgGTY2sV"
      },
      "outputs": [],
      "source": [
        "def beautify(sentence: str) -> str:\n",
        "    \"\"\"Removes useless spaces.\n",
        "    \"\"\"\n",
        "    punc = {'.', ',', ';'}\n",
        "    for p in punc:\n",
        "        sentence = sentence.replace(f' {p}', p)\n",
        "\n",
        "    links = {'-', \"'\"}\n",
        "    for l in links:\n",
        "        sentence = sentence.replace(f'{l} ', l)\n",
        "        sentence = sentence.replace(f' {l}', l)\n",
        "\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V9Q7qcvH2Chp"
      },
      "outputs": [],
      "source": [
        "def indices_terminated(\n",
        "        target: torch.FloatTensor,\n",
        "        eos_token: int\n",
        "    ) -> tuple:\n",
        "    \"\"\"Split the target sentences between the terminated and the non-terminated\n",
        "    sentence. Return the indices of those two groups.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        target: The sentences.\n",
        "            Shape of [batch_size, n_tokens].\n",
        "        eos_token: Value of the End-of-Sentence token.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        terminated: Indices of the terminated sentences (who's got the eos_token).\n",
        "            Shape of [n_terminated, ].\n",
        "        non-terminated: Indices of the unfinished sentences.\n",
        "            Shape of [batch_size-n_terminated, ].\n",
        "    \"\"\"\n",
        "    terminated = [i for i, t in enumerate(target) if eos_token in t]\n",
        "    non_terminated = [i for i, t in enumerate(target) if eos_token not in t]\n",
        "    return torch.LongTensor(terminated), torch.LongTensor(non_terminated)\n",
        "\n",
        "\n",
        "def append_beams(\n",
        "        target: torch.FloatTensor,\n",
        "        beams: torch.FloatTensor\n",
        "    ) -> torch.FloatTensor:\n",
        "    \"\"\"Add the beam tokens to the current sentences.\n",
        "    Duplicate the sentences so one token is added per beam per batch.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        target: Batch of unfinished sentences.\n",
        "            Shape of [batch_size, n_tokens].\n",
        "        beams: Batch of beams for each sentences.\n",
        "            Shape of [batch_size, n_beams].\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        target: Batch of sentences with one beam per sentence.\n",
        "            Shape of [batch_size * n_beams, n_tokens+1].\n",
        "    \"\"\"\n",
        "    batch_size, n_beams = beams.shape\n",
        "    n_tokens = target.shape[1]\n",
        "\n",
        "    target = einops.repeat(target, 'b t -> b c t', c=n_beams)  # [batch_size, n_beams, n_tokens]\n",
        "    beams = beams.unsqueeze(dim=2)  # [batch_size, n_beams, 1]\n",
        "\n",
        "    target = torch.cat((target, beams), dim=2)  # [batch_size, n_beams, n_tokens+1]\n",
        "    target = target.view(batch_size*n_beams, n_tokens+1)  # [batch_size * n_beams, n_tokens+1]\n",
        "    return target\n",
        "\n",
        "def beam_search(\n",
        "        model: nn.Module,\n",
        "        source: str,\n",
        "        src_vocab: Vocab,\n",
        "        tgt_vocab: Vocab,\n",
        "        src_tokenizer,\n",
        "        device: str,\n",
        "        beam_width: int,\n",
        "        max_target: int,\n",
        "        max_sentence_length: int,\n",
        "    ) -> list:\n",
        "    \"\"\"Do a beam search to produce probable translations.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The translation model. Assumes it produces linear score (before softmax).\n",
        "        source: The sentence to translate.\n",
        "        src_vocab: The source vocabulary.\n",
        "        tgt_vocab: The target vocabulary.\n",
        "        device: Device to which we make the inference.\n",
        "        beam_width: Number of top-k tokens we keep at each stage.\n",
        "        max_target: Maximum number of target sentences we keep at the end of each stage.\n",
        "        max_sentence_length: Maximum number of tokens for the translated sentence.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        sentences: List of sentences orderer by their likelihood.\n",
        "    \"\"\"\n",
        "    src_tokens = ['<bos>'] + src_tokenizer(source) + ['<eos>']\n",
        "    src_tokens = src_vocab(src_tokens)\n",
        "\n",
        "    tgt_tokens = ['<bos>']\n",
        "    tgt_tokens = tgt_vocab(tgt_tokens)\n",
        "\n",
        "    # To tensor and add unitary batch dimension\n",
        "    src_tokens = torch.LongTensor(src_tokens).to(device)\n",
        "    tgt_tokens = torch.LongTensor(tgt_tokens).unsqueeze(dim=0).to(device)\n",
        "    target_probs = torch.FloatTensor([1]).to(device)\n",
        "    model.to(device)\n",
        "\n",
        "    EOS_IDX = tgt_vocab['<eos>']\n",
        "    with torch.no_grad():\n",
        "        while tgt_tokens.shape[1] < max_sentence_length:\n",
        "            batch_size, n_tokens = tgt_tokens.shape\n",
        "\n",
        "            # Get next beams\n",
        "            src = einops.repeat(src_tokens, 't -> b t', b=tgt_tokens.shape[0])\n",
        "            predicted = model.forward(src, tgt_tokens)\n",
        "            predicted = torch.softmax(predicted, dim=-1)\n",
        "            probs, predicted = predicted[:, -1].topk(k=beam_width, dim=-1)\n",
        "\n",
        "            # Separe between terminated sentences and the others\n",
        "            idx_terminated, idx_not_terminated = indices_terminated(tgt_tokens, EOS_IDX)\n",
        "            idx_terminated, idx_not_terminated = idx_terminated.to(device), idx_not_terminated.to(device)\n",
        "\n",
        "            tgt_terminated = torch.index_select(tgt_tokens, dim=0, index=idx_terminated)\n",
        "            tgt_probs_terminated = torch.index_select(target_probs, dim=0, index=idx_terminated)\n",
        "\n",
        "            filter_t = lambda t: torch.index_select(t, dim=0, index=idx_not_terminated)\n",
        "            tgt_others = filter_t(tgt_tokens)\n",
        "            tgt_probs_others = filter_t(target_probs)\n",
        "            predicted = filter_t(predicted)\n",
        "            probs = filter_t(probs)\n",
        "\n",
        "            # Add the top tokens to the previous target sentences\n",
        "            tgt_others = append_beams(tgt_others, predicted)\n",
        "\n",
        "            # Add padding to terminated target\n",
        "            padd = torch.zeros((len(tgt_terminated), 1), dtype=torch.long, device=device)\n",
        "            tgt_terminated = torch.cat(\n",
        "                (tgt_terminated, padd),\n",
        "                dim=1\n",
        "            )\n",
        "\n",
        "            # Update each target sentence probabilities\n",
        "            tgt_probs_others = torch.repeat_interleave(tgt_probs_others, beam_width)\n",
        "            tgt_probs_others *= probs.flatten()\n",
        "            tgt_probs_terminated *= 0.999  # Penalize short sequences overtime\n",
        "\n",
        "            # Group up the terminated and the others\n",
        "            target_probs = torch.cat(\n",
        "                (tgt_probs_others, tgt_probs_terminated),\n",
        "                dim=0\n",
        "            )\n",
        "            tgt_tokens = torch.cat(\n",
        "                (tgt_others, tgt_terminated),\n",
        "                dim=0\n",
        "            )\n",
        "\n",
        "            # Keep only the top `max_target` target sentences\n",
        "            if target_probs.shape[0] <= max_target:\n",
        "                continue\n",
        "\n",
        "            target_probs, indices = target_probs.topk(k=max_target, dim=0)\n",
        "            tgt_tokens = torch.index_select(tgt_tokens, dim=0, index=indices)\n",
        "\n",
        "    sentences = []\n",
        "    for tgt_sentence in tgt_tokens:\n",
        "        tgt_sentence = list(tgt_sentence)[1:]  # Remove <bos> token\n",
        "        tgt_sentence = list(takewhile(lambda t: t != EOS_IDX, tgt_sentence))\n",
        "        tgt_sentence = ' '.join(tgt_vocab.lookup_tokens(tgt_sentence))\n",
        "        sentences.append(tgt_sentence)\n",
        "\n",
        "    sentences = [beautify(s) for s in sentences]\n",
        "\n",
        "    # Join the sentences with their likelihood\n",
        "    sentences = [(s, p.item()) for s, p in zip(sentences, target_probs)]\n",
        "    # Sort the sentences by their likelihood\n",
        "    sentences = [(s, p) for s, p in sorted(sentences, key=lambda k: k[1], reverse=True)]\n",
        "\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVr2FuDcZxC6"
      },
      "source": [
        "# Training loop\n",
        "This is a basic training loop code. It takes a big configuration dictionnary to avoid never ending arguments in the functions.\n",
        "We use [Weights and Biases](https://wandb.ai/) to log the trainings.\n",
        "It logs every training informations and model performances in the cloud.\n",
        "You have to create an account to use it. Every accounts are free for individuals or research teams."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0GK8VGrLlnP",
        "outputId": "63c21fd5-2236-4c38-b044-b9a9c1ee5058"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Déclaration unique de la fonction de lissage\n",
        "smoothing_function = nltk.translate.bleu_score.SmoothingFunction().method1\n",
        "\n",
        "def compute_bleu_score(prediction, reference):\n",
        "    \"\"\"\n",
        "    Compute BLEU score for a single sentence pair.\n",
        "\n",
        "    Args:\n",
        "    ----\n",
        "        prediction: Predicted sentence\n",
        "        reference: Reference (ground truth) sentence\n",
        "        tokenizer: Tokenization function to use\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "        bleu_score: BLEU score as a float (0-100)\n",
        "    \"\"\"\n",
        "    # Tokenize reference (as a list of reference sentences)\n",
        "    tokenized_reference = [fr_tokenizer(reference)]\n",
        "\n",
        "    # Tokenize prediction\n",
        "    tokenized_prediction = fr_tokenizer(prediction)\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu_score = nltk.translate.bleu_score.sentence_bleu(tokenized_reference, tokenized_prediction, smoothing_function=smoothing_function)\n",
        "\n",
        "    return bleu_score * 100  # Convert to percentage\n",
        "\n",
        "\n",
        "def compute_meteor_score(prediction, reference):\n",
        "    \"\"\"\n",
        "    Compute METEOR score using NLTK.\n",
        "\n",
        "    Args:\n",
        "    ----\n",
        "        predictions: List of predicted sentences\n",
        "        references: List of reference (ground truth) sentences\n",
        "\n",
        "    Returns:\n",
        "    -------\n",
        "        meteor_score_value: METEOR score as a float\n",
        "    \"\"\"\n",
        "    meteor_score = nltk.translate.meteor_score.meteor_score([fr_tokenizer(reference)], fr_tokenizer(prediction))\n",
        "    return meteor_score\n",
        "\n",
        "def eval_translation_metrics(model: nn.Module, val_dataset, config: dict) -> dict:\n",
        "    \"\"\"\n",
        "    Évalue les scores BLEU et METEOR sur l'ensemble du dataset de validation.\n",
        "\n",
        "    Args:\n",
        "        model: Le modèle de traduction.\n",
        "        val_dataset: Dataset de validation contenant des tuples (source, target).\n",
        "        config: Dictionnaire de configuration contenant notamment :\n",
        "            - device\n",
        "            - src_vocab, tgt_vocab\n",
        "            - src_tokenizer\n",
        "            - max_sequence_length\n",
        "    Returns:\n",
        "        Un dictionnaire avec les scores moyens 'bleu_score' et 'meteor_score'.\n",
        "    \"\"\"\n",
        "    print(\"Computing translation metrics...\")\n",
        "    device = config['device']\n",
        "    total_bleu = 0.0\n",
        "    total_meteor = 0.0\n",
        "    count = 0\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for source, target in val_dataset:\n",
        "            pred = None\n",
        "            # Beam search pour obtenir la prédiction\n",
        "            if config['search'] == 'greedy':\n",
        "                pred = greedy_search(\n",
        "                    model,\n",
        "                    source,\n",
        "                    config['src_vocab'],\n",
        "                    config['tgt_vocab'],\n",
        "                    config['src_tokenizer'],\n",
        "                    device,\n",
        "                    max_sentence_length=config['max_sequence_length'],\n",
        "                )\n",
        "            else:\n",
        "                pred, _ = beam_search(\n",
        "                    model,\n",
        "                    source,\n",
        "                    config['src_vocab'],\n",
        "                    config['tgt_vocab'],\n",
        "                    config['src_tokenizer'],\n",
        "                    device,\n",
        "                    beam_width=10,\n",
        "                    max_target=100,\n",
        "                    max_sentence_length=config['max_sequence_length'],\n",
        "                )[0]\n",
        "            total_bleu += compute_bleu_score(pred, target)\n",
        "            total_meteor += compute_meteor_score(pred, target)\n",
        "            count += 1\n",
        "\n",
        "    avg_bleu = total_bleu / count if count > 0 else 0.0\n",
        "    avg_meteor = total_meteor / count if count > 0 else 0.0\n",
        "\n",
        "    print(f\"Average BLEU Score: {avg_bleu:.2f}\")\n",
        "    print(f\"Average METEOR Score: {avg_meteor:.2f}\")\n",
        "    return {\"bleu_score\": avg_bleu, \"meteor_score\": avg_meteor}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Orj32G3we3U0",
        "outputId": "90edfe99-1da0-488a-ece5-74a01ef46fdb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU Score: 22.93\n",
            "METEOR Score: 0.54\n"
          ]
        }
      ],
      "source": [
        "# Tester les metriques\n",
        "target = \"Je n'oublierai jamais que j'ai passé un bon moment avec vous.\"\n",
        "pred = \"je n'oublierai jamais avoir du bon temps avec vous tous.\"\n",
        "\n",
        "bleu_score = compute_bleu_score(pred, target)\n",
        "print(f\"BLEU Score: {bleu_score:.2f}\")\n",
        "\n",
        "meteor_score_value = compute_meteor_score(pred, target)\n",
        "print(f\"METEOR Score: {meteor_score_value:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2I1C8pRXN8j"
      },
      "outputs": [],
      "source": [
        "def print_logs(dataset_type: str, logs: dict):\n",
        "    \"\"\"Print the logs.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        dataset_type: Either \"Train\", \"Eval\", \"Test\" type.\n",
        "        logs: Containing the metric's name and value.\n",
        "    \"\"\"\n",
        "    desc = [\n",
        "        f'{name}: {value:.2f}'\n",
        "        for name, value in logs.items()\n",
        "    ]\n",
        "    desc = '\\t'.join(desc)\n",
        "    desc = f'{dataset_type} -\\t' + desc\n",
        "    desc = desc.expandtabs(5)\n",
        "    print(desc)\n",
        "\n",
        "\n",
        "def topk_accuracy(\n",
        "        real_tokens: torch.FloatTensor,\n",
        "        probs_tokens: torch.FloatTensor,\n",
        "        k: int,\n",
        "        tgt_pad_idx: int,\n",
        "    ) -> torch.FloatTensor:\n",
        "    \"\"\"Compute the top-k accuracy.\n",
        "    We ignore the PAD tokens.\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        real_tokens: Real tokens of the target sentence.\n",
        "            Shape of [batch_size * n_tokens].\n",
        "        probs_tokens: Tokens probability predicted by the model.\n",
        "            Shape of [batch_size * n_tokens, n_target_vocabulary].\n",
        "        k: Top-k accuracy threshold.\n",
        "        src_pad_idx: Source padding index value.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        acc: Scalar top-k accuracy value.\n",
        "    \"\"\"\n",
        "    total = (real_tokens != tgt_pad_idx).sum()\n",
        "\n",
        "    _, pred_tokens = probs_tokens.topk(k=k, dim=-1)  # [batch_size * n_tokens, k]\n",
        "    real_tokens = einops.repeat(real_tokens, 'b -> b k', k=k)  # [batch_size * n_tokens, k]\n",
        "\n",
        "    good = (pred_tokens == real_tokens) & (real_tokens != tgt_pad_idx)\n",
        "    acc = good.sum() / total\n",
        "    return acc\n",
        "\n",
        "\n",
        "def loss_batch(\n",
        "        model: nn.Module,\n",
        "        source: torch.LongTensor,\n",
        "        target: torch.LongTensor,\n",
        "        config: dict,\n",
        "    )-> dict:\n",
        "    \"\"\"Compute the metrics associated with this batch.\n",
        "    The metrics are:\n",
        "        - loss\n",
        "        - top-1 accuracy\n",
        "        - top-5 accuracy\n",
        "        - top-10 accuracy\n",
        "\n",
        "    Args\n",
        "    ----\n",
        "        model: The model to train.\n",
        "        source: Batch of source tokens.\n",
        "            Shape of [batch_size, n_src_tokens].\n",
        "        target: Batch of target tokens.\n",
        "            Shape of [batch_size, n_tgt_tokens].\n",
        "        config: Additional parameters.\n",
        "\n",
        "    Output\n",
        "    ------\n",
        "        metrics: Dictionnary containing evaluated metrics on this batch.\n",
        "    \"\"\"\n",
        "    device = config['device']\n",
        "    loss_fn = config['loss'].to(device)\n",
        "    metrics = dict()\n",
        "\n",
        "    source, target = source.to(device), target.to(device)\n",
        "    target_in, target_out = target[:, :-1], target[:, 1:]\n",
        "\n",
        "    # Loss\n",
        "    pred = model(source, target_in)  # [batch_size, n_tgt_tokens-1, n_vocab]\n",
        "    pred = pred.view(-1, pred.shape[2])  # [batch_size * (n_tgt_tokens - 1), n_vocab]\n",
        "    target_out = target_out.flatten()  # [batch_size * (n_tgt_tokens - 1),]\n",
        "    metrics['loss'] = loss_fn(pred, target_out)\n",
        "\n",
        "    # Accuracy - we ignore the padding predictions\n",
        "    for k in [1, 5, 10]:\n",
        "        metrics[f'top-{k}'] = topk_accuracy(target_out, pred, k, config['tgt_pad_idx'])\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def eval_model(model: nn.Module, dataloader: DataLoader, config: dict) -> dict:\n",
        "    \"\"\"Evaluate the model on the given dataloader.\n",
        "    \"\"\"\n",
        "    device = config['device']\n",
        "    logs = defaultdict(list)\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for source, target in dataloader:\n",
        "            metrics = loss_batch(model, source, target, config)\n",
        "            for name, value in metrics.items():\n",
        "                logs[name].append(value.cpu().item())\n",
        "\n",
        "    for name, values in logs.items():\n",
        "        logs[name] = np.mean(values)\n",
        "    return logs\n",
        "\n",
        "\n",
        "def train_model(model: nn.Module, config: dict):\n",
        "    \"\"\"Train the model in a teacher forcing manner.\n",
        "    \"\"\"\n",
        "    train_loader, val_loader = config['train_loader'], config['val_loader']\n",
        "    train_dataset, val_dataset = train_loader.dataset.dataset, val_loader.dataset.dataset\n",
        "    optimizer = config['optimizer']\n",
        "    clip = config['clip']\n",
        "    device = config['device']\n",
        "\n",
        "    columns = ['epoch']\n",
        "    for mode in ['train', 'validation']:\n",
        "        columns += [\n",
        "            f'{mode} - {colname}'\n",
        "            for colname in ['source', 'target', 'predicted', 'likelihood', 'bleu', 'meteor']\n",
        "        ]\n",
        "    log_table = wandb.Table(columns=columns)\n",
        "\n",
        "\n",
        "    print(f'Starting training for {config[\"epochs\"]} epochs, using {device}.')\n",
        "    for e in range(config['epochs']):\n",
        "        print(f'\\nEpoch {e+1}')\n",
        "\n",
        "        model.to(device)\n",
        "        model.train()\n",
        "        logs = defaultdict(list)\n",
        "\n",
        "        for batch_id, (source, target) in enumerate(train_loader):\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            metrics = loss_batch(model, source, target, config)\n",
        "            loss = metrics['loss']\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            for name, value in metrics.items():\n",
        "                logs[name].append(value.cpu().item())  # Don't forget the '.item' to free the cuda memory\n",
        "\n",
        "            if batch_id % config['log_every'] == 0:\n",
        "                # Moyenne des logs du batch courant\n",
        "                for name, value in logs.items():\n",
        "                    logs[name] = np.mean(value)\n",
        "\n",
        "                train_logs = {\n",
        "                    f'Train - {m}': v\n",
        "                    for m, v in logs.items()\n",
        "                }\n",
        "                wandb.log(train_logs)\n",
        "                logs = defaultdict(list)\n",
        "\n",
        "        # Logs après la boucle d'entraînement\n",
        "        if len(logs) != 0:\n",
        "            for name, value in logs.items():\n",
        "                logs[name] = np.mean(value)\n",
        "            train_logs = {\n",
        "                f'Train - {m}': v\n",
        "                for m, v in logs.items()\n",
        "            }\n",
        "        else:\n",
        "            logs = {\n",
        "                m.split(' - ')[1]: v\n",
        "                for m, v in train_logs.items()\n",
        "            }\n",
        "\n",
        "        print_logs('Train', logs)\n",
        "\n",
        "        # Évaluation sur le jeu de validation (métriques de loss et top-k)\n",
        "        logs = eval_model(model, val_loader, config)\n",
        "        print_logs('Eval', logs)\n",
        "        val_logs = {\n",
        "            f'Validation - {m}': v\n",
        "            for m, v in logs.items()\n",
        "        }\n",
        "\n",
        "\n",
        "        # Calcul des métriques de traduction sur l'ensemble du dataset de validation\n",
        "        # Commenté car vraiment trop long !\n",
        "        # Il faudrait modifier beam_search pour traiter des batches de phrases\n",
        "        # translation_metrics = eval_translation_metrics(model, val_dataset, config)\n",
        "\n",
        "        # Merge dictionnaries\n",
        "        logs = {\n",
        "            **train_logs,\n",
        "            **val_logs,\n",
        "            #'Validation - bleu_score': translation_metrics['bleu_score'],\n",
        "            #'Validation - meteor_score': translation_metrics['meteor_score']\n",
        "        }\n",
        "        wandb.log(logs)  # Upload to the WandB cloud\n",
        "\n",
        "        # Table logs\n",
        "        train_source, train_target = train_dataset[ torch.randint(len(train_dataset), (1,)) ]\n",
        "        val_source, val_target = val_dataset[ torch.randint(len(val_dataset), (1,)) ]\n",
        "\n",
        "        val_pred, val_prob, train_pred, train_prob = None, None, None, None\n",
        "\n",
        "        if config['search'] == 'greedy':\n",
        "            val_pred = greedy_search(\n",
        "                model,\n",
        "                val_source,\n",
        "                config['src_vocab'],\n",
        "                config['tgt_vocab'],\n",
        "                config['src_tokenizer'],\n",
        "                device,\n",
        "                max_sentence_length=config['max_sequence_length'],\n",
        "            )\n",
        "            train_pred = greedy_search(\n",
        "                model,\n",
        "                train_source,\n",
        "                config['src_vocab'],\n",
        "                config['tgt_vocab'],\n",
        "                config['src_tokenizer'],\n",
        "                device,\n",
        "                max_sentence_length=config['max_sequence_length'],\n",
        "            )\n",
        "        else:\n",
        "            val_pred, val_prob = beam_search(\n",
        "                model,\n",
        "                val_source,\n",
        "                config['src_vocab'],\n",
        "                config['tgt_vocab'],\n",
        "                config['src_tokenizer'],\n",
        "                device,  # It can take a lot of VRAM\n",
        "                beam_width=10,\n",
        "                max_target=100,\n",
        "                max_sentence_length=config['max_sequence_length'],\n",
        "            )[0]\n",
        "            train_pred, train_prob = beam_search(\n",
        "                model,\n",
        "                train_source,\n",
        "                config['src_vocab'],\n",
        "                config['tgt_vocab'],\n",
        "                config['src_tokenizer'],\n",
        "                device,  # It can take a lot of VRAM\n",
        "                beam_width=10,\n",
        "                max_target=100,\n",
        "                max_sentence_length=config['max_sequence_length'],\n",
        "            )[0]\n",
        "\n",
        "        print(val_source)\n",
        "        print(val_pred)\n",
        "\n",
        "        # Calcul de quelques scores BLUE et METEOR\n",
        "        train_bleu = compute_bleu_score(train_pred, train_target)\n",
        "        train_meteor = compute_meteor_score(train_pred, train_target)\n",
        "        val_bleu = compute_bleu_score(val_pred, val_target)\n",
        "        val_meteor = compute_meteor_score(val_pred, val_target)\n",
        "\n",
        "        data = [\n",
        "            e + 1,\n",
        "            train_source, train_target, train_pred, train_prob, train_bleu, train_meteor,\n",
        "            val_source, val_target, val_pred, val_prob, val_bleu, val_meteor,\n",
        "        ]\n",
        "        log_table.add_data(*data)\n",
        "\n",
        "    # Log the table at the end of the training\n",
        "    wandb.log({'Model predictions': log_table})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YImgxCWjlWni"
      },
      "source": [
        "# Training the models\n",
        "We can now finally train the models.\n",
        "Choose the right hyperparameters, play with them and try to find\n",
        "ones that lead to good models and good training curves.\n",
        "Try to reach a loss under 1.0.\n",
        "\n",
        "So you know, it is possible to get descent results with approximately 20 epochs.\n",
        "With CUDA enabled, one epoch, even on a big model with a big dataset, shouldn't last more than 10 minutes.\n",
        "A normal epoch is between 1 to 5 minutes.\n",
        "\n",
        "*This is considering Colab Pro, we should try using free Colab to get better estimations.*\n",
        "\n",
        "---\n",
        "\n",
        "To test your implementations, it is easier to try your models\n",
        "in a CPU instance. Indeed, Colab reduces your GPU instances priority\n",
        "with the time you recently past using GPU instances. It would be\n",
        "sad to consume all your GPU time on implementation testing.\n",
        "Moreover, you should try your models on small datasets and with a small number of parameters.\n",
        "For exemple, you could set:\n",
        "```\n",
        "MAX_SEQ_LEN = 10\n",
        "MIN_TOK_FREQ = 20\n",
        "dim_embedding = 40\n",
        "dim_hidden = 60\n",
        "n_layers = 1\n",
        "```\n",
        "\n",
        "You usually don't want to log anything onto WandB when testing your implementation.\n",
        "To deactivate WandB without having to change any line of code, you can type `!wandb offline` in a cell.\n",
        "\n",
        "Once you have rightly implemented the models, you can train bigger models on bigger datasets.\n",
        "When you do this, do not forget to change the runtime as GPU (and use `!wandb online`)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WriScTUEsRHr",
        "outputId": "64075245-4b0c-433e-f00f-573ad7d2dfef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "Thu Mar 27 05:50:52 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   39C    P8             16W /   72W |       3MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Checking GPU and logging to wandb\n",
        "# Key : 3d4435602abbbbedc3a6db344dfddef6fe581a4d\n",
        "!wandb login --relogin\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tv4SinyOJ2Gb",
        "outputId": "c57ae5db-c721-44e2-9816-7b888ecd78c3"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqmpxnO1lgDy",
        "outputId": "dd4e0b49-d86d-489f-e704-1dd15cf2f4c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "English vocabulary size: 11,043\n",
            "French vocabulary size: 17,264\n",
            "\n",
            "Training examples: 209,459\n",
            "Validation examples: 23,274\n"
          ]
        }
      ],
      "source": [
        "# Instanciate the datasets\n",
        "\n",
        "MAX_SEQ_LEN = 60\n",
        "MIN_TOK_FREQ = 2\n",
        "train_dataset, val_dataset = build_datasets(\n",
        "    MAX_SEQ_LEN,\n",
        "    MIN_TOK_FREQ,\n",
        "    en_tokenizer,\n",
        "    fr_tokenizer,\n",
        "    train,\n",
        "    valid,\n",
        ")\n",
        "\n",
        "\n",
        "print(f'English vocabulary size: {len(train_dataset.en_vocab):,}')\n",
        "print(f'French vocabulary size: {len(train_dataset.fr_vocab):,}')\n",
        "\n",
        "print(f'\\nTraining examples: {len(train_dataset):,}')\n",
        "print(f'Validation examples: {len(val_dataset):,}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ywFEpplOU5dn"
      },
      "outputs": [],
      "source": [
        "# Build the model, the dataloaders, optimizer and the loss function\n",
        "# Log every hyperparameters and arguments into the config dictionnary\n",
        "\n",
        "# Configuration par défaut\n",
        "config_default = {\n",
        "    # General parameters\n",
        "    'epochs': 10, # 5\n",
        "    'batch_size': 128,# 128\n",
        "    'lr': 1e-3,\n",
        "    'betas': (0.9, 0.99),\n",
        "    'clip': 5,\n",
        "    'device': 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "\n",
        "    # Model parameters\n",
        "    'n_tokens_src': len(train_dataset.en_vocab),\n",
        "    'n_tokens_tgt': len(train_dataset.fr_vocab),\n",
        "    'n_heads': 4,\n",
        "    'dim_embedding': 196, # 196\n",
        "    'dim_hidden': 256, # 256\n",
        "    'n_layers': 3,\n",
        "    'dropout': 0.1,\n",
        "\n",
        "    # Others\n",
        "    'max_sequence_length': MAX_SEQ_LEN,\n",
        "    'min_token_freq': MIN_TOK_FREQ,\n",
        "    'src_vocab': train_dataset.en_vocab,\n",
        "    'tgt_vocab': train_dataset.fr_vocab,\n",
        "    'src_tokenizer': en_tokenizer,\n",
        "    'tgt_tokenizer': fr_tokenizer,\n",
        "    'src_pad_idx': train_dataset.en_vocab['<pad>'],\n",
        "    'tgt_pad_idx': train_dataset.fr_vocab['<pad>'],\n",
        "    'seed': 0,\n",
        "    'log_every': 50,  # Number of batches between each wandb logs\n",
        "    'search': 'beam',  # 'greedy' or 'beam'\n",
        "}\n",
        "\n",
        "torch.manual_seed(config_default['seed'])\n",
        "\n",
        "config_default['train_loader'] = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=config_default['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config_default['src_pad_idx'], config_default['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "config_default['val_loader'] = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=config_default['batch_size'],\n",
        "    shuffle=True,\n",
        "    collate_fn=lambda batch: generate_batch(batch, config_default['src_pad_idx'], config_default['tgt_pad_idx'])\n",
        ")\n",
        "\n",
        "# Uncomment code block to select model to train here!\n",
        "\"\"\"\n",
        "model = TranslationRNN(\n",
        "    config_default['n_tokens_src'],\n",
        "    config_default['n_tokens_tgt'],\n",
        "    config_default['dim_embedding'],\n",
        "    config_default['dim_hidden'],\n",
        "    config_default['n_layers'],\n",
        "    config_default['dropout'],\n",
        "    config_default['src_pad_idx'],\n",
        "    config_default['tgt_pad_idx'],\n",
        "    'RNN'\n",
        ")\n",
        "\"\"\"\n",
        "model = TranslationTransformer(\n",
        "    config_default['n_tokens_src'],\n",
        "    config_default['n_tokens_tgt'],\n",
        "    config_default['n_heads'],\n",
        "    config_default['dim_embedding'],\n",
        "    config_default['dim_hidden'],\n",
        "    config_default['n_layers'],\n",
        "    config_default['dropout'],\n",
        "    config_default['src_pad_idx'],\n",
        "    config_default['tgt_pad_idx'],\n",
        ")\n",
        "\n",
        "def setup_model(model_type, config, print=False):\n",
        "    model = None\n",
        "    if model_type == 'RNN':\n",
        "        model = TranslationRNN(\n",
        "            config['n_tokens_src'],\n",
        "            config['n_tokens_tgt'],\n",
        "            config['dim_embedding'],\n",
        "            config['dim_hidden'],\n",
        "            config['n_layers'],\n",
        "            config['dropout'],\n",
        "            config['src_pad_idx'],\n",
        "            config['tgt_pad_idx'],\n",
        "            'RNN'\n",
        "        )\n",
        "    elif model_type == 'GRU':\n",
        "        model = TranslationRNN(\n",
        "            config['n_tokens_src'],\n",
        "            config['n_tokens_tgt'],\n",
        "            config['dim_embedding'],\n",
        "            config['dim_hidden'],\n",
        "            config['n_layers'],\n",
        "            config['dropout'],\n",
        "            config['src_pad_idx'],\n",
        "            config['tgt_pad_idx'],\n",
        "            'GRU'\n",
        "        )\n",
        "    elif model_type == 'Transformer':\n",
        "        model = TranslationTransformer(\n",
        "            config['n_tokens_src'],\n",
        "            config['n_tokens_tgt'],\n",
        "            config['n_heads'],\n",
        "            config['dim_embedding'],\n",
        "            config['dim_hidden'],\n",
        "            config['n_layers'],\n",
        "            config['dropout'],\n",
        "            config['src_pad_idx'],\n",
        "            config['tgt_pad_idx'],\n",
        "        )\n",
        "\n",
        "    config['optimizer'] = optim.Adam(\n",
        "        model.parameters(),\n",
        "        lr=config['lr'],\n",
        "        betas=config['betas'],\n",
        "    )\n",
        "\n",
        "    weight_classes = torch.ones(config['n_tokens_tgt'], dtype=torch.float)\n",
        "    weight_classes[config['tgt_vocab']['<unk>']] = 0.1  # Lower the importance of that class\n",
        "    config['loss'] = nn.CrossEntropyLoss(\n",
        "        weight=weight_classes,\n",
        "        ignore_index=config['tgt_pad_idx'],  # We do not have to learn those\n",
        "    )\n",
        "\n",
        "    if print:\n",
        "        print(summary(\n",
        "            model,\n",
        "            input_size=[\n",
        "                (config['batch_size'], config['max_sequence_length']),\n",
        "                (config['batch_size'], config['max_sequence_length'])\n",
        "            ],\n",
        "            dtypes=[torch.long, torch.long],\n",
        "            depth=3,\n",
        "        ))\n",
        "\n",
        "    return model, config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1781
        },
        "id": "maOTVtk4acxD",
        "outputId": "b6514c20-1115-4df5-98c1-88a8aa18739e"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "W&B online. Running your script from this directory will now sync to the cloud.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_063720-b78vil95</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/b78vil95' target=\"_blank\">Transformer - Default</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/b78vil95' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/b78vil95</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.76     top-1: 0.65    top-5: 0.84    top-10: 0.88\n",
            "Eval -    loss: 1.51     top-1: 0.68    top-5: 0.86    top-10: 0.89\n",
            "I don't think it'll be difficult to do that.\n",
            "je ne pense pas que ça sera difficile de faire ça.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.42     top-1: 0.70    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.24     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "Tom already knew about it.\n",
            "tom en savait déjà.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.24     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.13     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "All I want to do is finish what I started.\n",
            "tout ce que je veux faire est ce que j'ai commencé.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.18     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.06     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Don't bother waking me up at 4:00 a.m. I don't plan to go fishing tomorrow.\n",
            "ne t'inquiète pas à me lever à quatre heures que je ne prévois pas à pêcher demain.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.08     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.02     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "I'd like to buy a house, but I can't afford to.\n",
            "j'aimerais acheter une maison, mais je ne peux pas me permettre.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.07     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 0.98     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "I'm sure we can figure out a way to get this done.\n",
            "je suis sûr que nous pouvons trouver un moyen de le faire.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.03     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.96     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "He broke his leg skiing.\n",
            "il a cassé la jambe.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 0.97     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.93     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "We need the rain.\n",
            "nous avons besoin de la pluie.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.97     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.92     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Was anybody with you?\n",
            "quelqu'un avec toi ?\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.94     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.91     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Tom said he'd like to visit Boston.\n",
            "tom a dit qu'il aimerait rendre visite à boston.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▇▆▅▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▃▅▅▅▆▆▆▆▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇▇▇█▇██▇███</td></tr><tr><td>Train - top-10</td><td>▁▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████▇█████████</td></tr><tr><td>Train - top-5</td><td>▁▄▅▆▇▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.93872</td></tr><tr><td>Train - top-1</td><td>0.77393</td></tr><tr><td>Train - top-10</td><td>0.95267</td></tr><tr><td>Train - top-5</td><td>0.93098</td></tr><tr><td>Validation - loss</td><td>0.90597</td></tr><tr><td>Validation - top-1</td><td>0.78459</td></tr><tr><td>Validation - top-10</td><td>0.95407</td></tr><tr><td>Validation - top-5</td><td>0.93361</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Transformer - Default</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/b78vil95' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/b78vil95</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_063720-b78vil95/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "!wandb online  # online / offline / disabled to activate, deactivate or turn off WandB logging\n",
        "\n",
        "model, config = setup_model('Transformer', {**config_default})\n",
        "\n",
        "with wandb.init(\n",
        "        config={**config},\n",
        "        project='INF8225 - TP3',  # Title of your project\n",
        "        group=f\"Default\", # In what group of runs do you want this run to be in?\n",
        "        name='Transformer - Default',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "vwu4JctgNzLF",
        "outputId": "4fd2dee6-f605-40f1-e1c0-d721144268e1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_065831-8we1mway</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/8we1mway' target=\"_blank\">RNN - Default</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/8we1mway' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/8we1mway</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.74     top-1: 0.50    top-5: 0.68    top-10: 0.74\n",
            "Eval -    loss: 2.63     top-1: 0.50    top-5: 0.69    top-10: 0.75\n",
            "Put that back on the table.\n",
            "mets-le.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.48     top-1: 0.52    top-5: 0.72    top-10: 0.78\n",
            "Eval -    loss: 2.37     top-1: 0.54    top-5: 0.73    top-10: 0.78\n",
            "Amputation is needed.\n",
            "l'homme est très occupé.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.31     top-1: 0.54    top-5: 0.74    top-10: 0.80\n",
            "Eval -    loss: 2.25     top-1: 0.55    top-5: 0.74    top-10: 0.80\n",
            "The Giants beat the Lions yesterday.\n",
            "le garçon s'est arrêté.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.22     top-1: 0.56    top-5: 0.75    top-10: 0.81\n",
            "Eval -    loss: 2.16     top-1: 0.56    top-5: 0.76    top-10: 0.81\n",
            "I want my hammer back.\n",
            "je veux moi ma chambre.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.17     top-1: 0.56    top-5: 0.76    top-10: 0.82\n",
            "Eval -    loss: 2.11     top-1: 0.57    top-5: 0.76    top-10: 0.82\n",
            "This is a pencil.\n",
            "c'est un désastre.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.08     top-1: 0.57    top-5: 0.77    top-10: 0.82\n",
            "Eval -    loss: 2.07     top-1: 0.57    top-5: 0.77    top-10: 0.83\n",
            "I hope you enjoyed your stay with us.\n",
            "j'espère que tu peux rester un peu plus lentement.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 2.05     top-1: 0.57    top-5: 0.78    top-10: 0.83\n",
            "Eval -    loss: 2.04     top-1: 0.58    top-5: 0.77    top-10: 0.83\n",
            "How long are you planning to stay?\n",
            "combien de temps allez-vous rester ici ?\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 2.04     top-1: 0.57    top-5: 0.78    top-10: 0.83\n",
            "Eval -    loss: 2.02     top-1: 0.58    top-5: 0.78    top-10: 0.83\n",
            "Why don't you just study a little harder?\n",
            "pourquoi ne me dites-vous pas une tasse de thé ?\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 2.01     top-1: 0.58    top-5: 0.78    top-10: 0.84\n",
            "Eval -    loss: 2.00     top-1: 0.58    top-5: 0.78    top-10: 0.83\n",
            "Instead of cutting down on cigarettes, why don't you just give them up?\n",
            "à son avis   ?\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.97     top-1: 0.58    top-5: 0.79    top-10: 0.84\n",
            "Eval -    loss: 1.99     top-1: 0.58    top-5: 0.78    top-10: 0.84\n",
            "We were successful because we worked hard.\n",
            "nous étions fatigués depuis trois ans.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▆▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▄▄▅▅▅▅▅▅▅▆▇▆▇▆▇▇▆▇▇▇▇█▇▇▇▇█▇▇██▇█████▇█</td></tr><tr><td>Train - top-10</td><td>▁▃▃▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████</td></tr><tr><td>Train - top-5</td><td>▁▃▃▃▃▄▅▆▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇██▇██████</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.97396</td></tr><tr><td>Train - top-1</td><td>0.58229</td></tr><tr><td>Train - top-10</td><td>0.84179</td></tr><tr><td>Train - top-5</td><td>0.78625</td></tr><tr><td>Validation - loss</td><td>1.98879</td></tr><tr><td>Validation - top-1</td><td>0.58448</td></tr><tr><td>Validation - top-10</td><td>0.83736</td></tr><tr><td>Validation - top-5</td><td>0.78324</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">RNN - Default</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/8we1mway' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/8we1mway</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_065831-8we1mway/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, config = setup_model('RNN', {**config_default})\n",
        "\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',  # Title of your project\n",
        "        group=f\"Default\", # In what group of runs do you want this run to be in?\n",
        "        name='RNN - Default',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "jXJC36IiNzXV",
        "outputId": "83677052-a4ff-4a66-8e86-f26d6a76cc28"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_073124-yknw9552</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/yknw9552' target=\"_blank\">GRU - Default</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/yknw9552' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/yknw9552</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.90     top-1: 0.62    top-5: 0.82    top-10: 0.86\n",
            "Eval -    loss: 1.75     top-1: 0.64    top-5: 0.83    top-10: 0.87\n",
            "That's much more important.\n",
            "c'est plus important.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.52     top-1: 0.68    top-5: 0.87    top-10: 0.90\n",
            "Eval -    loss: 1.42     top-1: 0.69    top-5: 0.87    top-10: 0.91\n",
            "What is it you do?\n",
            "que fais-tu faire ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.33     top-1: 0.70    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.29     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "I caught the flu.\n",
            "j'ai attrapé la grippe.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.26     top-1: 0.71    top-5: 0.89    top-10: 0.93\n",
            "Eval -    loss: 1.22     top-1: 0.72    top-5: 0.90    top-10: 0.92\n",
            "Drop by sometime.\n",
            "les commandes se sont différentes.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.16     top-1: 0.73    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.18     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "It's all true.\n",
            "tout est vrai.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.08     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.16     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "I think that Tom should learn to speak French.\n",
            "je pense que tom devrait apprendre à parler français.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.03     top-1: 0.75    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 1.15     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "I don't wear glasses.\n",
            "je ne porte pas de lunettes.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.01     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 1.13     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "Don't forget me.\n",
            "ne m'oubliez pas.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.98     top-1: 0.76    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 1.13     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "I must leave on Monday.\n",
            "je dois partir lundi.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.96     top-1: 0.76    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 1.12     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Who did you go with?\n",
            "avec qui êtes-vous allé ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▇▆▃▃▃▃▃▂▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▆▇▇▇▇▇▇▇▇████▇█████████████████████████</td></tr><tr><td>Train - top-10</td><td>▁▄▄▆▆▇▇▇▇▇▇▇▇██▇▇▇██████████████████████</td></tr><tr><td>Train - top-5</td><td>▁▅▅▆▆▇▇▇▇▇██████████████████████████████</td></tr><tr><td>Validation - loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▆▇▇▇████</td></tr><tr><td>Validation - top-10</td><td>▁▅▆▇▇█████</td></tr><tr><td>Validation - top-5</td><td>▁▅▆▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.96162</td></tr><tr><td>Train - top-1</td><td>0.76048</td></tr><tr><td>Train - top-10</td><td>0.95289</td></tr><tr><td>Train - top-5</td><td>0.92828</td></tr><tr><td>Validation - loss</td><td>1.12046</td></tr><tr><td>Validation - top-1</td><td>0.74436</td></tr><tr><td>Validation - top-10</td><td>0.93516</td></tr><tr><td>Validation - top-5</td><td>0.90877</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">GRU - Default</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/yknw9552' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/yknw9552</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_073124-yknw9552/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, config = setup_model('GRU', {**config_default})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',  # Title of your project\n",
        "        group=f\"Default\", # In what group of runs do you want this run to be in?\n",
        "        name='GRU - Default',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Nombre de couches (défaut=3 --> 2 et 4)"
      ],
      "metadata": {
        "id": "ECb1nDKARxTZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "RnarCMZ4R3hB",
        "outputId": "ca6390b7-5ba7-4819-acaa-2f610aa9b722"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_083821-y7u4exjq</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/y7u4exjq' target=\"_blank\">Transformer - Layers 2</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/y7u4exjq' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/y7u4exjq</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.78     top-1: 0.65    top-5: 0.83    top-10: 0.87\n",
            "Eval -    loss: 1.58     top-1: 0.67    top-5: 0.85    top-10: 0.89\n",
            "Iron is hard.\n",
            "le fer est difficile.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.51     top-1: 0.68    top-5: 0.87    top-10: 0.90\n",
            "Eval -    loss: 1.30     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "What's obvious is that you're not very happy here.\n",
            "quel est évident que vous n'êtes pas très heureuse ici.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.35     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.19     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "A good many people were there.\n",
            "beaucoup de gens étaient là-bas.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.27     top-1: 0.72    top-5: 0.90    top-10: 0.92\n",
            "Eval -    loss: 1.11     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "I wish I could buy a motorcycle.\n",
            "j'aimerais pouvoir acheter une moto.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.17     top-1: 0.73    top-5: 0.91    top-10: 0.93\n",
            "Eval -    loss: 1.07     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Can eating just vegetables help you lose weight?\n",
            "manger manger des légumes pour vous perdre du poids ?\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.10     top-1: 0.74    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Tom is handsome, isn't he?\n",
            "tom est beau, n'est-ce pas ?\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.09     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.00     top-1: 0.77    top-5: 0.92    top-10: 0.95\n",
            "Let me take this one.\n",
            "laissez-moi prendre ceci.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 0.99     top-1: 0.77    top-5: 0.92    top-10: 0.95\n",
            "I believe it'll change my life.\n",
            "je crois que ça changera ma vie.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.01     top-1: 0.75    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.97     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "We'll take every precaution.\n",
            "nous allons prendre tous les soirs.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.00     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.95     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "That was close.\n",
            "c'était proche.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>Train - top-10</td><td>▁▃▄▅▆▇▇▇▇▇▇▇▇▇▇▇████████████████████████</td></tr><tr><td>Train - top-5</td><td>▁▃▃▅▆▆▇▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇▇▇██</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇▇▇██</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.99815</td></tr><tr><td>Train - top-1</td><td>0.76353</td></tr><tr><td>Train - top-10</td><td>0.94853</td></tr><tr><td>Train - top-5</td><td>0.92336</td></tr><tr><td>Validation - loss</td><td>0.94991</td></tr><tr><td>Validation - top-1</td><td>0.7786</td></tr><tr><td>Validation - top-10</td><td>0.95063</td></tr><tr><td>Validation - top-5</td><td>0.92911</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Transformer - Layers 2</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/y7u4exjq' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/y7u4exjq</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_083821-y7u4exjq/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, config = setup_model('Transformer', {**config_default, 'n_layers': 2})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',\n",
        "        group=f\"Layers\",\n",
        "        name='Transformer - Layers 2',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "iR3rPgpYSAGM",
        "outputId": "4713f63d-3c8c-4585-8407-44093bc5671c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_085641-a2japqtg</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/a2japqtg' target=\"_blank\">RNN - Layers 2</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/a2japqtg' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/a2japqtg</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.74     top-1: 0.50    top-5: 0.68    top-10: 0.74\n",
            "Eval -    loss: 2.63     top-1: 0.50    top-5: 0.69    top-10: 0.75\n",
            "What are you trying to hide from me?\n",
            "que faites-vous ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.48     top-1: 0.52    top-5: 0.71    top-10: 0.77\n",
            "Eval -    loss: 2.38     top-1: 0.53    top-5: 0.72    top-10: 0.78\n",
            "It really was hard.\n",
            "ça n'aime pas.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.30     top-1: 0.55    top-5: 0.74    top-10: 0.80\n",
            "Eval -    loss: 2.24     top-1: 0.55    top-5: 0.74    top-10: 0.80\n",
            "That's no excuse.\n",
            "ce n'est pas la mienne.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.20     top-1: 0.55    top-5: 0.75    top-10: 0.81\n",
            "Eval -    loss: 2.16     top-1: 0.56    top-5: 0.76    top-10: 0.81\n",
            "I went to the bank to take out money.\n",
            "je suis allé au travail.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.10     top-1: 0.56    top-5: 0.77    top-10: 0.83\n",
            "Eval -    loss: 2.11     top-1: 0.57    top-5: 0.77    top-10: 0.82\n",
            "Zoology and botany deal with the study of life.\n",
            "les yeux de ses enfants et les chats.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.09     top-1: 0.57    top-5: 0.77    top-10: 0.82\n",
            "Eval -    loss: 2.07     top-1: 0.57    top-5: 0.77    top-10: 0.83\n",
            "I saw a koala for the first time.\n",
            "j'ai vu un étranger hier soir.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 2.04     top-1: 0.57    top-5: 0.78    top-10: 0.83\n",
            "Eval -    loss: 2.05     top-1: 0.58    top-5: 0.78    top-10: 0.83\n",
            "We saw the sun sink below the horizon.\n",
            "nous nous connaissons tous les jours.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.98     top-1: 0.58    top-5: 0.78    top-10: 0.84\n",
            "Eval -    loss: 2.02     top-1: 0.58    top-5: 0.78    top-10: 0.83\n",
            "Don't forget your stuff.\n",
            "n'oublie pas ton français.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.97     top-1: 0.58    top-5: 0.79    top-10: 0.84\n",
            "Eval -    loss: 2.00     top-1: 0.58    top-5: 0.78    top-10: 0.84\n",
            "I'm thinking about changing careers.\n",
            "je pense souvent.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.95     top-1: 0.59    top-5: 0.79    top-10: 0.85\n",
            "Eval -    loss: 1.98     top-1: 0.59    top-5: 0.78    top-10: 0.84\n",
            "Are you not a doctor?\n",
            "n'allez-vous pas ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▆▆▆▆▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▂▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▃▄▅▅▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇█▇▇▇█████████████</td></tr><tr><td>Train - top-10</td><td>▁▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████████████████████</td></tr><tr><td>Train - top-5</td><td>▁▁▂▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█████</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▃▅▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.94528</td></tr><tr><td>Train - top-1</td><td>0.58641</td></tr><tr><td>Train - top-10</td><td>0.8469</td></tr><tr><td>Train - top-5</td><td>0.78988</td></tr><tr><td>Validation - loss</td><td>1.98301</td></tr><tr><td>Validation - top-1</td><td>0.58567</td></tr><tr><td>Validation - top-10</td><td>0.83725</td></tr><tr><td>Validation - top-5</td><td>0.78485</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">RNN - Layers 2</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/a2japqtg' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/a2japqtg</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_085641-a2japqtg/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, config = setup_model('RNN', {**config_default, 'n_layers': 2})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',\n",
        "        group=f\"Layers\",\n",
        "        name='RNN - Layers 2',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "UvvKRdzrSAV8",
        "outputId": "afcd997c-8df2-4f7b-a711-b78534d8ac85"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_092139-jgtoib0o</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/jgtoib0o' target=\"_blank\">GRU - Layers 2</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/jgtoib0o' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/jgtoib0o</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.78     top-1: 0.64    top-5: 0.83    top-10: 0.88\n",
            "Eval -    loss: 1.68     top-1: 0.65    top-5: 0.84    top-10: 0.88\n",
            "We have to crack down on illegal trading.\n",
            "nous devons prendre du temps.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.47     top-1: 0.69    top-5: 0.87    top-10: 0.91\n",
            "Eval -    loss: 1.38     top-1: 0.70    top-5: 0.88    top-10: 0.91\n",
            "I don't think that anything's broken.\n",
            "je ne pense pas que ce soit quoi que ce soit.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.28     top-1: 0.71    top-5: 0.89    top-10: 0.93\n",
            "Eval -    loss: 1.27     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "It wasn't a problem.\n",
            "ce n'était pas un problème.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.17     top-1: 0.73    top-5: 0.91    top-10: 0.93\n",
            "Eval -    loss: 1.23     top-1: 0.72    top-5: 0.90    top-10: 0.92\n",
            "You're in a restricted area.\n",
            "tu es dans une zone.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.11     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.18     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Let's do this properly.\n",
            "faisons-le correctement.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.08     top-1: 0.74    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.16     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "I didn't vote.\n",
            "je n'ai pas voté.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.02     top-1: 0.75    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 1.15     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "Tom's lying.\n",
            "tom ment.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 0.97     top-1: 0.76    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 1.14     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "Will it clear up soon?\n",
            "est-ce que ça va bientôt se lever bientôt ?\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.94     top-1: 0.76    top-5: 0.93    top-10: 0.96\n",
            "Eval -    loss: 1.13     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "I just got promoted.\n",
            "je viens d'être promu.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.91     top-1: 0.77    top-5: 0.93    top-10: 0.96\n",
            "Eval -    loss: 1.13     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "I'll go.\n",
            "j'irai.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▇▇▅▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▃▃▄▄▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇█▇▇█▇██████████████</td></tr><tr><td>Train - top-10</td><td>▁▃▄▅▅▆▆▆▇▇▇▇▇▇█▇▇▇▇▇███▇▇███████████████</td></tr><tr><td>Train - top-5</td><td>▁▂▅▅▅▆▆▆▆▇▇▇▇▇▇█▇▇▇████▇████████████████</td></tr><tr><td>Validation - loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▆▆▇▇████</td></tr><tr><td>Validation - top-10</td><td>▁▅▆▇▇█████</td></tr><tr><td>Validation - top-5</td><td>▁▅▆▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.90529</td></tr><tr><td>Train - top-1</td><td>0.77107</td></tr><tr><td>Train - top-10</td><td>0.95809</td></tr><tr><td>Train - top-5</td><td>0.93427</td></tr><tr><td>Validation - loss</td><td>1.12878</td></tr><tr><td>Validation - top-1</td><td>0.74227</td></tr><tr><td>Validation - top-10</td><td>0.93494</td></tr><tr><td>Validation - top-5</td><td>0.90809</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">GRU - Layers 2</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/jgtoib0o' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/jgtoib0o</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_092139-jgtoib0o/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, config = setup_model('GRU', {**config_default, 'n_layers': 2})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',\n",
        "        group=f\"Layers\",\n",
        "        name='GRU - Layers 2',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "hhy47aMcTC2F",
        "outputId": "3deb473f-53ed-4de2-b9b1-372239f8637e"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_100923-7yw2c3tr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/7yw2c3tr' target=\"_blank\">Transformer - Layers 4</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/7yw2c3tr' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/7yw2c3tr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.64     top-1: 0.67    top-5: 0.85    top-10: 0.89\n",
            "Eval -    loss: 1.47     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "Let me tell you what I think.\n",
            "laisse-moi vous dire ce que je pense.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.36     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.21     top-1: 0.73    top-5: 0.90    top-10: 0.92\n",
            "We have a traitor among us.\n",
            "nous avons un dictionnaire.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.24     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.10     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "She taught him how to play the piano.\n",
            "elle l'a appris à jouer du piano.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.14     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.03     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "I know Tom was shaken.\n",
            "je sais que tom a été secoué.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.06     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 0.99     top-1: 0.77    top-5: 0.92    top-10: 0.94\n",
            "Please write to me once in a while.\n",
            "veuillez m'écrire une fois par un moment.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.02     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.96     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "We're unlucky.\n",
            "nous sommes malchanceux.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 0.95     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.94     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "It's not as hard as you think.\n",
            "ce n'est pas aussi dur que vous le pensez.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 0.97     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.91     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "You're my kid's teacher.\n",
            "vous êtes l'institutrice de mon enfant.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.91     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.90     top-1: 0.79    top-5: 0.93    top-10: 0.95\n",
            "The government must make fundamental changes.\n",
            "le gouvernement doit faire des changements.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.90     top-1: 0.78    top-5: 0.93    top-10: 0.96\n",
            "Eval -    loss: 0.88     top-1: 0.79    top-5: 0.94    top-10: 0.95\n",
            "I had hardly left home when it began raining.\n",
            "j'avais presque quitté la maison quand il a commencé à pleuvoir.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▃▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>Train - top-10</td><td>▁▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇▇██▇▇████</td></tr><tr><td>Train - top-5</td><td>▁▃▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█████▇█████████</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▆▆▇▇▇███</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.9022</td></tr><tr><td>Train - top-1</td><td>0.78012</td></tr><tr><td>Train - top-10</td><td>0.95584</td></tr><tr><td>Train - top-5</td><td>0.9345</td></tr><tr><td>Validation - loss</td><td>0.88382</td></tr><tr><td>Validation - top-1</td><td>0.78955</td></tr><tr><td>Validation - top-10</td><td>0.95489</td></tr><tr><td>Validation - top-5</td><td>0.93565</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Transformer - Layers 4</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/7yw2c3tr' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/7yw2c3tr</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_100923-7yw2c3tr/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, config = setup_model('Transformer', {**config_default, 'n_layers': 4})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',\n",
        "        group=f\"Layers\",\n",
        "        name='Transformer - Layers 4',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "RyCBK4yRTC2F",
        "outputId": "a45568f1-ea53-4744-9c38-a7fb36125185"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_103429-bppcwzzv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/bppcwzzv' target=\"_blank\">RNN - Layers 4</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/bppcwzzv' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/bppcwzzv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.84     top-1: 0.48    top-5: 0.66    top-10: 0.72\n",
            "Eval -    loss: 2.73     top-1: 0.49    top-5: 0.67    top-10: 0.73\n",
            "The concert will start at 2:30.\n",
            "la porte était malade.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 2.57     top-1: 0.51    top-5: 0.70    top-10: 0.76\n",
            "Eval -    loss: 2.44     top-1: 0.52    top-5: 0.71    top-10: 0.77\n",
            "Tom doesn't drink tea.\n",
            "tom ne travaille pas.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 2.38     top-1: 0.53    top-5: 0.73    top-10: 0.78\n",
            "Eval -    loss: 2.31     top-1: 0.54    top-5: 0.73    top-10: 0.79\n",
            "His grades have improved significantly.\n",
            "son chien s'est calmé.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 2.31     top-1: 0.54    top-5: 0.73    top-10: 0.79\n",
            "Eval -    loss: 2.23     top-1: 0.55    top-5: 0.74    top-10: 0.80\n",
            "Is it free?\n",
            "est-ce bon ?\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 2.20     top-1: 0.55    top-5: 0.75    top-10: 0.81\n",
            "Eval -    loss: 2.17     top-1: 0.56    top-5: 0.75    top-10: 0.81\n",
            "Would you mind if I ate a piece of this pie?\n",
            "verrais-tu une telle chose ?\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 2.18     top-1: 0.55    top-5: 0.75    top-10: 0.81\n",
            "Eval -    loss: 2.14     top-1: 0.56    top-5: 0.76    top-10: 0.81\n",
            "We demanded that she should make up for the loss.\n",
            "nous espérons que tom était en retard.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 2.16     top-1: 0.55    top-5: 0.76    top-10: 0.82\n",
            "Eval -    loss: 2.11     top-1: 0.57    top-5: 0.76    top-10: 0.82\n",
            "That's all you can do.\n",
            "c'est ce que tu veux.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 2.15     top-1: 0.55    top-5: 0.76    top-10: 0.82\n",
            "Eval -    loss: 2.10     top-1: 0.57    top-5: 0.76    top-10: 0.82\n",
            "I'm going to reconsider it.\n",
            "je vais faire ça.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 2.10     top-1: 0.56    top-5: 0.77    top-10: 0.82\n",
            "Eval -    loss: 2.08     top-1: 0.57    top-5: 0.77    top-10: 0.82\n",
            "No one told us.\n",
            "personne ne nous a demandé.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 2.07     top-1: 0.57    top-5: 0.77    top-10: 0.83\n",
            "Eval -    loss: 2.06     top-1: 0.57    top-5: 0.77    top-10: 0.83\n",
            "It'll soon be sunset.\n",
            "ce sera bientôt.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▂▄▅▅▅▆▆▆▆▇▇▇▇▇▇█▇▇▇████████████████████</td></tr><tr><td>Train - top-10</td><td>▁▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇█▇██</td></tr><tr><td>Train - top-5</td><td>▁▃▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████████████████</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▁▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇████</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▇▇████</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>2.07397</td></tr><tr><td>Train - top-1</td><td>0.56648</td></tr><tr><td>Train - top-10</td><td>0.82702</td></tr><tr><td>Train - top-5</td><td>0.76892</td></tr><tr><td>Validation - loss</td><td>2.06442</td></tr><tr><td>Validation - top-1</td><td>0.5714</td></tr><tr><td>Validation - top-10</td><td>0.82524</td></tr><tr><td>Validation - top-5</td><td>0.76994</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">RNN - Layers 4</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/bppcwzzv' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/bppcwzzv</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_103429-bppcwzzv/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, config = setup_model('RNN', {**config_default, 'n_layers': 4})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',\n",
        "        group=f\"Layers\",\n",
        "        name='RNN - Layers 4',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "JE_WkVQnTC2G",
        "outputId": "8378882b-04a7-4746-ecf7-e48f6eb6390e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_111537-8mo7lfma</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/8mo7lfma' target=\"_blank\">GRU - Layers 4</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/8mo7lfma' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/8mo7lfma</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 2.03     top-1: 0.61    top-5: 0.80    top-10: 0.85\n",
            "Eval -    loss: 1.90     top-1: 0.62    top-5: 0.81    top-10: 0.86\n",
            "Do you often become dizzy if you get up from bed quickly?\n",
            "est-ce que tu es souvent souvent vite, si tu te peux ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.61     top-1: 0.66    top-5: 0.85    top-10: 0.89\n",
            "Eval -    loss: 1.49     top-1: 0.68    top-5: 0.86    top-10: 0.90\n",
            "Will you help them?\n",
            "vous allez-vous les aider ?\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.43     top-1: 0.69    top-5: 0.87    top-10: 0.91\n",
            "Eval -    loss: 1.35     top-1: 0.70    top-5: 0.88    top-10: 0.91\n",
            "I feel like a drink.\n",
            "j'ai envie d'un verre.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.34     top-1: 0.70    top-5: 0.88    top-10: 0.92\n",
            "Eval -    loss: 1.27     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Tom is an activist.\n",
            "tom est un adulte.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.20     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.22     top-1: 0.73    top-5: 0.89    top-10: 0.92\n",
            "How far is it from here to there?\n",
            "combien y a-t-il d'ici ?\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.18     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.20     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Don't tell me who I like.\n",
            "ne me dites pas qui j'aime.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.12     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.18     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "He's used to speaking in public.\n",
            "il est habitué à parler en public.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.07     top-1: 0.74    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.16     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "Why haven't we begun?\n",
            "pourquoi n'avons-nous pas commencé ?\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.06     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.15     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "Wonderful!\n",
            "c'est merveilleux !\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.04     top-1: 0.75    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 1.15     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "You didn't understand me.\n",
            "tu ne m'as pas comprise.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▆▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▃▄▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇████████████████</td></tr><tr><td>Train - top-10</td><td>▁▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇███▇▇▇███████████████</td></tr><tr><td>Train - top-5</td><td>▁▂▃▄▅▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇████████████</td></tr><tr><td>Validation - loss</td><td>█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▅▆▇▇▇████</td></tr><tr><td>Validation - top-10</td><td>▁▅▆▇▇▇████</td></tr><tr><td>Validation - top-5</td><td>▁▅▆▇▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.03977</td></tr><tr><td>Train - top-1</td><td>0.74703</td></tr><tr><td>Train - top-10</td><td>0.94569</td></tr><tr><td>Train - top-5</td><td>0.91877</td></tr><tr><td>Validation - loss</td><td>1.14603</td></tr><tr><td>Validation - top-1</td><td>0.74018</td></tr><tr><td>Validation - top-10</td><td>0.93113</td></tr><tr><td>Validation - top-5</td><td>0.90425</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">GRU - Layers 4</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/8mo7lfma' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/8mo7lfma</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_111537-8mo7lfma/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, config = setup_model('GRU', {**config_default, 'n_layers': 4})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',\n",
        "        group=f\"Layers\",\n",
        "        name='GRU - Layers 4',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer : Dimension embedding (défaut=196 --> 128 et 256)"
      ],
      "metadata": {
        "id": "KgTO_p40TMdL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "wtiwSC4oTdNs",
        "outputId": "49676b3d-4b75-4c88-eade-f8741cf50765"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_124223-x87jddix</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/x87jddix' target=\"_blank\">Transformer - Embed 128</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/x87jddix' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/x87jddix</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.94     top-1: 0.63    top-5: 0.82    top-10: 0.86\n",
            "Eval -    loss: 1.73     top-1: 0.65    top-5: 0.83    top-10: 0.87\n",
            "I will look the other way.\n",
            "j'aurai l'autre moyen.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.56     top-1: 0.68    top-5: 0.86    top-10: 0.89\n",
            "Eval -    loss: 1.38     top-1: 0.70    top-5: 0.88    top-10: 0.91\n",
            "Tom was very friendly.\n",
            "tom était très amical.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.39     top-1: 0.70    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.24     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Some things aren't going to change.\n",
            "certaines choses ne vont pas changer.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.29     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.16     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "Tom has three ex-wives.\n",
            "tom a trois kilos.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.25     top-1: 0.72    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.11     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "You did the right thing.\n",
            "tu as fait la bonne chose.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.18     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.07     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "In any case, it's always my fault.\n",
            "en tout cas, c'est toujours ma faute.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.13     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.04     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "I'll never tell anyone who you really are.\n",
            "je ne dirai jamais à personne qui vous êtes vraiment.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.13     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.01     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "How cold is it in Boston right now?\n",
            "quel froid est-il à boston en ce moment   ?\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 1.08     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.00     top-1: 0.77    top-5: 0.92    top-10: 0.94\n",
            "Could you turn the volume down?\n",
            "pourriez-vous baisser le volume ?\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 1.05     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 0.98     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "I'm optimistic.\n",
            "je suis optimiste.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▆▆▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▂▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>Train - top-10</td><td>▁▃▄▄▄▅▆▆▆▇▇▇▇▇█▇▇▇▇▇████████████████████</td></tr><tr><td>Train - top-5</td><td>▁▂▃▃▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███▇▇▇▇████████████</td></tr><tr><td>Validation - loss</td><td>█▅▃▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▆▆▇▇████</td></tr><tr><td>Validation - top-5</td><td>▁▄▆▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>1.05492</td></tr><tr><td>Train - top-1</td><td>0.75375</td></tr><tr><td>Train - top-10</td><td>0.94392</td></tr><tr><td>Train - top-5</td><td>0.91857</td></tr><tr><td>Validation - loss</td><td>0.98072</td></tr><tr><td>Validation - top-1</td><td>0.77098</td></tr><tr><td>Validation - top-10</td><td>0.94679</td></tr><tr><td>Validation - top-5</td><td>0.92524</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Transformer - Embed 128</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/x87jddix' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/x87jddix</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_124223-x87jddix/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, config = setup_model('Transformer', {**config_default, 'dim_embedding': 128})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',\n",
        "        group=f\"Embedding\",\n",
        "        name='Transformer - Embed 128',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, config = setup_model('Transformer', {**config_default, 'dim_embedding': 256})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',\n",
        "        group=f\"Embedding\",\n",
        "        name='Transformer - Embed 256',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "S01F1AcGTjNI",
        "outputId": "eae07da6-2fda-4f63-bacb-3013618bbe44"
      },
      "execution_count": 84,
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_130257-j1w04t0f</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/j1w04t0f' target=\"_blank\">Transformer - Embed 256</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/j1w04t0f' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/j1w04t0f</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.60     top-1: 0.67    top-5: 0.86    top-10: 0.89\n",
            "Eval -    loss: 1.43     top-1: 0.70    top-5: 0.87    top-10: 0.90\n",
            "I promise you that I won't ever leave you.\n",
            "je te promets que je ne te laisserai jamais.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.34     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.18     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "That isn't exactly a top priority.\n",
            "ce n'est pas exactement.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.21     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.09     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "He wasn't there last week.\n",
            "il n'était pas là la semaine dernière.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.14     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.03     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Everyone quieted down.\n",
            "tout le monde descendit.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.05     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 0.99     top-1: 0.77    top-5: 0.92    top-10: 0.95\n",
            "I have a job I have to do.\n",
            "j'ai un travail à faire.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.02     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.95     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Will we arrive in time?\n",
            "arrivera-t-on à l'heure ?\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 0.94     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.94     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I'm going through my closet to find clothes to give to charity.\n",
            "je vais à travers mon placard pour trouver des vêtements de charité.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 0.92     top-1: 0.77    top-5: 0.93    top-10: 0.96\n",
            "Eval -    loss: 0.91     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "He's doing his German homework.\n",
            "il fait ses devoirs allemand.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.91     top-1: 0.78    top-5: 0.93    top-10: 0.96\n",
            "Eval -    loss: 0.90     top-1: 0.79    top-5: 0.93    top-10: 0.95\n",
            "Don't make me do this.\n",
            "ne m'obligez pas à faire ça.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.89     top-1: 0.78    top-5: 0.94    top-10: 0.96\n",
            "Eval -    loss: 0.89     top-1: 0.79    top-5: 0.94    top-10: 0.96\n",
            "Can I join you?\n",
            "puis-je vous rejoindre ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▄▄▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▂▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇█▇▇▇██▇▇▇█████</td></tr><tr><td>Train - top-10</td><td>▁▇▇█████████████████████████████████████</td></tr><tr><td>Train - top-5</td><td>▁▅▆▆▇▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▇▇████</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.88667</td></tr><tr><td>Train - top-1</td><td>0.78143</td></tr><tr><td>Train - top-10</td><td>0.9588</td></tr><tr><td>Train - top-5</td><td>0.93706</td></tr><tr><td>Validation - loss</td><td>0.89191</td></tr><tr><td>Validation - top-1</td><td>0.78783</td></tr><tr><td>Validation - top-10</td><td>0.95577</td></tr><tr><td>Validation - top-5</td><td>0.93613</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Transformer - Embed 256</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/j1w04t0f' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/j1w04t0f</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_130257-j1w04t0f/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer : Dimension cachée (défaut=256 --> 128 et 512)"
      ],
      "metadata": {
        "id": "mi5w-2fsTnk5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "Xo6RgkXSTnk5",
        "outputId": "484db6bc-9097-4824-a251-9e5e67421928"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_132503-f1lq3sxi</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/f1lq3sxi' target=\"_blank\">Transformer - Hidden_dim 128</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/f1lq3sxi' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/f1lq3sxi</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.75     top-1: 0.65    top-5: 0.84    top-10: 0.88\n",
            "Eval -    loss: 1.54     top-1: 0.68    top-5: 0.86    top-10: 0.89\n",
            "I suppose you think I'm crazy.\n",
            "je suppose que vous pensez que je suis fou.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.46     top-1: 0.69    top-5: 0.87    top-10: 0.91\n",
            "Eval -    loss: 1.28     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "She has a good heart.\n",
            "elle a un bon cœur.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.28     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.16     top-1: 0.74    top-5: 0.90    top-10: 0.93\n",
            "I'm still thirsty.\n",
            "je suis encore soif.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.21     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.08     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "We can all dream.\n",
            "nous pouvons tous rêve.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.13     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.03     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "He loves her.\n",
            "il l'aime.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.08     top-1: 0.75    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 1.00     top-1: 0.77    top-5: 0.92    top-10: 0.94\n",
            "It's a very contagious virus.\n",
            "c'est un très improbable.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.03     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.98     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "That's the maximum.\n",
            "c'est le <unk>.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.03     top-1: 0.75    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.95     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "It is very hot in the summer in Japan.\n",
            "il fait très chaud dans l'été au japon.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.98     top-1: 0.76    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.94     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I caught the flu.\n",
            "j'ai attrapé la grippe.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.99     top-1: 0.76    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.91     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "She tore his letter to pieces.\n",
            "elle déchira sa lettre en morceaux.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▅▅▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▂▃▃▄▅▅▅▆▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇</td></tr><tr><td>Train - top-10</td><td>▁▁▂▄▄▅▆▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇████████</td></tr><tr><td>Train - top-5</td><td>▁▁▂▂▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇███████████</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.99283</td></tr><tr><td>Train - top-1</td><td>0.76369</td></tr><tr><td>Train - top-10</td><td>0.94973</td></tr><tr><td>Train - top-5</td><td>0.92539</td></tr><tr><td>Validation - loss</td><td>0.91482</td></tr><tr><td>Validation - top-1</td><td>0.78228</td></tr><tr><td>Validation - top-10</td><td>0.95307</td></tr><tr><td>Validation - top-5</td><td>0.93253</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Transformer - Hidden_dim 128</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/f1lq3sxi' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/f1lq3sxi</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_132503-f1lq3sxi/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, config = setup_model('Transformer', {**config_default, 'dim_hidden': 128})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',\n",
        "        group=f\"Hidden_dim\",\n",
        "        name='Transformer - Hidden_dim 128',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, config = setup_model('Transformer', {**config_default, 'dim_hidden': 512})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',\n",
        "        group=f\"Hidden_dim\",\n",
        "        name='Transformer - Hidden_dim 512',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1163
        },
        "id": "Js1zR6tWTnk6",
        "outputId": "55602ad3-60d4-4127-fab4-a28dedf58aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_134618-jjyf5tdu</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/jjyf5tdu' target=\"_blank\">Transformer - Hidden_dim 512</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/jjyf5tdu' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/jjyf5tdu</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.67     top-1: 0.66    top-5: 0.85    top-10: 0.88\n",
            "Eval -    loss: 1.46     top-1: 0.69    top-5: 0.87    top-10: 0.90\n",
            "Have you ever been diagnosed with asthma?\n",
            "as-tu déjà diagnostiqué avec impatience ?\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.34     top-1: 0.71    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.20     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "You don't seem too sure.\n",
            "tu n'as pas l'air trop sûr.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.20     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.09     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Is there anything you'd like to share?\n",
            "y a-t-il quelque chose que tu voudrais partager ?\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.13     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.03     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "You and I have the same idea.\n",
            "vous et moi avons l'idée.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.06     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "Eval -    loss: 0.99     top-1: 0.77    top-5: 0.92    top-10: 0.95\n",
            "He was blazing with anger.\n",
            "il était en colère après colère.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.01     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.96     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Can I pay on credit?\n",
            "puis-je payer à crédit ?\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 0.98     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.93     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "He became a famous singer.\n",
            "il est devenu chanteur célèbre.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 0.95     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.91     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I only have one thing left to do.\n",
            "je n'ai qu'une seule chose à faire.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.93     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.90     top-1: 0.79    top-5: 0.93    top-10: 0.95\n",
            "They started to sell a new type of car in Tokyo.\n",
            "ils commencèrent à vendre un nouveau genre de voiture à tokyo.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.89     top-1: 0.78    top-5: 0.94    top-10: 0.96\n",
            "Eval -    loss: 0.89     top-1: 0.79    top-5: 0.94    top-10: 0.95\n",
            "This computer runs on batteries.\n",
            "cet ordinateur court sur piles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer : Nombre de tetes d'attention (défaut=4 --> 2 et 8)"
      ],
      "metadata": {
        "id": "sjnlA558UGTh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "3GRZ60D8UGTi",
        "outputId": "8e09528a-99ff-4a74-b0ea-c5d44ed42ae3"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_140751-ldk6ajzn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/ldk6ajzn' target=\"_blank\">Transformer - Heads 2</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/ldk6ajzn' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/ldk6ajzn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.77     top-1: 0.65    top-5: 0.84    top-10: 0.88\n",
            "Eval -    loss: 1.54     top-1: 0.68    top-5: 0.86    top-10: 0.89\n",
            "Don't be angry.\n",
            "ne sois pas en colère.\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.44     top-1: 0.69    top-5: 0.87    top-10: 0.91\n",
            "Eval -    loss: 1.27     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "I don't know where this came from.\n",
            "je ne sais pas d'où est venu.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.27     top-1: 0.72    top-5: 0.89    top-10: 0.92\n",
            "Eval -    loss: 1.15     top-1: 0.74    top-5: 0.91    top-10: 0.93\n",
            "It's only temporary.\n",
            "c'est seulement temporaire.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.20     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.08     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "You had no right to do that.\n",
            "tu avais raison de faire ça.\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.14     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.03     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "This is the first time I've ever written a letter in French.\n",
            "c'est la première fois que j'ai jamais écrit en français.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.09     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.00     top-1: 0.77    top-5: 0.92    top-10: 0.95\n",
            "You always ask too many questions.\n",
            "tu as toujours posé trop de questions.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.03     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.98     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Tom wasn't traveling.\n",
            "tom n'allait pas voyager.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 1.02     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.95     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Tom makes me feel safe.\n",
            "tom me fait sentir en sécurité.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.96     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.94     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "Why can't you do it?\n",
            "pourquoi ne peux-tu pas le faire ?\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.97     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.92     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "We still need your help.\n",
            "nous avons toujours besoin de votre aide.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▅▄▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train - top-1</td><td>▁▃▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████████████████████</td></tr><tr><td>Train - top-10</td><td>▁▂▄▄▄▅▅▆▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇██████▇██████</td></tr><tr><td>Train - top-5</td><td>▁▇▇▇▇▇▇█████████████████████████████████</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▆▆▇▇▇███</td></tr><tr><td>Validation - top-5</td><td>▁▄▆▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.96595</td></tr><tr><td>Train - top-1</td><td>0.76592</td></tr><tr><td>Train - top-10</td><td>0.95136</td></tr><tr><td>Train - top-5</td><td>0.92792</td></tr><tr><td>Validation - loss</td><td>0.92205</td></tr><tr><td>Validation - top-1</td><td>0.78133</td></tr><tr><td>Validation - top-10</td><td>0.9526</td></tr><tr><td>Validation - top-5</td><td>0.93217</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Transformer - Heads 2</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/ldk6ajzn' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/ldk6ajzn</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_140751-ldk6ajzn/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "model, config = setup_model('Transformer', {**config_default, 'n_heads': 2})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',\n",
        "        group=f\"Heads\",\n",
        "        name='Transformer - Heads 2',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model, config = setup_model('Transformer', {**config_default, 'n_heads': 7})\n",
        "with wandb.init(\n",
        "        config=config,\n",
        "        project='INF8225 - TP3',\n",
        "        group=f\"Heads\",\n",
        "        name='Transformer - Heads 7',\n",
        "        save_code=True,\n",
        "    ):\n",
        "    train_model(model, config)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1763
        },
        "id": "DclygqUbUGTi",
        "outputId": "1e30b85f-7056-4787-c1c5-d297a9c0acc1"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250327_143038-xscj14xb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/xscj14xb' target=\"_blank\">Transformer - Heads 8</a></strong> to <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/xscj14xb' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/xscj14xb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training for 10 epochs, using cuda.\n",
            "\n",
            "Epoch 1\n",
            "Train -   loss: 1.71     top-1: 0.66    top-5: 0.84    top-10: 0.88\n",
            "Eval -    loss: 1.49     top-1: 0.69    top-5: 0.86    top-10: 0.90\n",
            "Wait here.\n",
            "attends ici !\n",
            "\n",
            "Epoch 2\n",
            "Train -   loss: 1.37     top-1: 0.71    top-5: 0.88    top-10: 0.91\n",
            "Eval -    loss: 1.23     top-1: 0.73    top-5: 0.90    top-10: 0.92\n",
            "I don't want to be exploited.\n",
            "je ne veux pas être <unk>.\n",
            "\n",
            "Epoch 3\n",
            "Train -   loss: 1.22     top-1: 0.73    top-5: 0.90    top-10: 0.93\n",
            "Eval -    loss: 1.12     top-1: 0.75    top-5: 0.91    top-10: 0.93\n",
            "That's not uncommon.\n",
            "ce n'est pas rare.\n",
            "\n",
            "Epoch 4\n",
            "Train -   loss: 1.13     top-1: 0.74    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.05     top-1: 0.76    top-5: 0.92    top-10: 0.94\n",
            "How many books do you read a month?\n",
            "combien de livres lis-tu un mois ?\n",
            "\n",
            "Epoch 5\n",
            "Train -   loss: 1.10     top-1: 0.75    top-5: 0.91    top-10: 0.94\n",
            "Eval -    loss: 1.01     top-1: 0.77    top-5: 0.92    top-10: 0.94\n",
            "Paramedics arrived on the scene within minutes.\n",
            "on est arrivé sur les scène dans les minutes.\n",
            "\n",
            "Epoch 6\n",
            "Train -   loss: 1.02     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.98     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "I'll see to it immediately.\n",
            "je vais voir immédiatement.\n",
            "\n",
            "Epoch 7\n",
            "Train -   loss: 1.00     top-1: 0.76    top-5: 0.92    top-10: 0.95\n",
            "Eval -    loss: 0.94     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "The earth is a lot larger than the moon.\n",
            "la terre est beaucoup plus grosse que la lune.\n",
            "\n",
            "Epoch 8\n",
            "Train -   loss: 0.97     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.93     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I've already told everybody to go home.\n",
            "j'ai déjà dit à tout le monde d'aller à la maison.\n",
            "\n",
            "Epoch 9\n",
            "Train -   loss: 0.96     top-1: 0.77    top-5: 0.93    top-10: 0.95\n",
            "Eval -    loss: 0.92     top-1: 0.78    top-5: 0.93    top-10: 0.95\n",
            "I'm no friend of yours.\n",
            "je ne suis pas de mes amies.\n",
            "\n",
            "Epoch 10\n",
            "Train -   loss: 0.93     top-1: 0.78    top-5: 0.93    top-10: 0.96\n",
            "Eval -    loss: 0.90     top-1: 0.79    top-5: 0.93    top-10: 0.95\n",
            "What happened here?\n",
            "que s'est passé ici ?\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>█▇▇▆▅▄▄▄▄▄▃▂▂▂▂▂▃▂▃▂▂▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▂</td></tr><tr><td>Train - top-1</td><td>▁▂▂▃▃▄▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇████████████</td></tr><tr><td>Train - top-10</td><td>▁▄▅▆▇▇▇▇▇▇▇▇▇▇▇█▇█▇█████████████████████</td></tr><tr><td>Train - top-5</td><td>▁▃▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███▇█████████████████</td></tr><tr><td>Validation - loss</td><td>█▅▄▃▂▂▂▁▁▁</td></tr><tr><td>Validation - top-1</td><td>▁▄▅▆▇▇▇███</td></tr><tr><td>Validation - top-10</td><td>▁▄▅▆▇▇████</td></tr><tr><td>Validation - top-5</td><td>▁▄▅▆▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Train - loss</td><td>0.92624</td></tr><tr><td>Train - top-1</td><td>0.77509</td></tr><tr><td>Train - top-10</td><td>0.9551</td></tr><tr><td>Train - top-5</td><td>0.93314</td></tr><tr><td>Validation - loss</td><td>0.89733</td></tr><tr><td>Validation - top-1</td><td>0.78672</td></tr><tr><td>Validation - top-10</td><td>0.95409</td></tr><tr><td>Validation - top-5</td><td>0.93452</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">Transformer - Heads 8</strong> at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/xscj14xb' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3/runs/xscj14xb</a><br> View project at: <a href='https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3' target=\"_blank\">https://wandb.ai/gabriel-abenhaim-polytechnique-montr-al/INF8225%20-%20TP3</a><br>Synced 5 W&B file(s), 1 media file(s), 2 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250327_143038-xscj14xb/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9PFIyvKUefdV",
        "outputId": "7e8282b6-83a6-41a7-fe04-fd388190f677"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0. (30.06611%) \t je serai là dans dix minutes. \t B: 80.91067115702212 \t M: 0.981329690346084\n",
            "1. (8.69694%) \t j'y serai dans dix minutes. \t B: 17.56720523942792 \t M: 0.6147540983606558\n",
            "2. (4.23965%) \t je serai là dans quelques minutes. \t B: 43.472087194499146 \t M: 0.7934426229508196\n",
            "3. (2.92384%) \t je serai là dans environ dix minutes. \t B: 41.11336169005197 \t M: 0.9498207885304659\n",
            "4. (2.53553%) \t je serai là dans 10 minutes. \t B: 43.472087194499146 \t M: 0.7934426229508196\n"
          ]
        }
      ],
      "source": [
        "sentence = \"I'll be there in 10 minutes\"\n",
        "target = \"je serai là dans dix minutes\"\n",
        "\n",
        "preds = beam_search(\n",
        "    model,\n",
        "    sentence,\n",
        "    config['src_vocab'],\n",
        "    config['tgt_vocab'],\n",
        "    config['src_tokenizer'],\n",
        "    config['device'],\n",
        "    beam_width=10,\n",
        "    max_target=100,\n",
        "    max_sentence_length=config['max_sequence_length']\n",
        ")[:5]\n",
        "\n",
        "for i, (translation, likelihood) in enumerate(preds):\n",
        "    print(f'{i}. ({likelihood*100:.5f}%) \\t {translation} \\t B: {compute_bleu_score(translation, target)} \\t M: {compute_meteor_score(translation, target)}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test avec greed\n",
        "preds_greedy = greedy_search(\n",
        "    model,\n",
        "    sentence,\n",
        "    config['src_vocab'],\n",
        "    config['tgt_vocab'],\n",
        "    config['src_tokenizer'],\n",
        "    config['device'],\n",
        "    max_sentence_length=config['max_sequence_length']\n",
        ")\n",
        "print(f\"{preds_greedy}  B: {compute_bleu_score(preds_greedy, target)} \\t M: {compute_meteor_score(preds_greedy, target)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "snM3vl-hL-jo",
        "outputId": "9439df65-26d2-478f-af05-a3585f4ba192"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "je serai là dans dix minutes .  B: 80.91067115702212 \t M: 0.981329690346084\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHhixEEGzWRR"
      },
      "source": [
        "# Grading:\n",
        "\n",
        "# Implementations (50 points total)\n",
        "\n",
        "10 Points for your implementation of the GRU\n",
        "\n",
        "40 Points for your implementaiton of the Transformer components\n",
        "\n",
        "# Questions (12 points, 1 point each)\n",
        "1. Explain the differences between Vanilla RNN, GRU-RNN, Encoder-Decoder Transformer and Decoder-Only Transformer.\n",
        "2. Why is positionnal encoding necessary in Transformers and not in RNNs?\n",
        "3. Describe the preprocessing process. Detail how the initial dataset is processed before being fed to the translation models.\n",
        "4. What is teacher forcing, and how is it used in Transformer training? How does the decoder input differ?\n",
        "5. How are the two types of mask important to the attention mechanism (causal and padding) and how do they work? How do they differ between the encoder and decoder?\n",
        "6. What is a causal mask, and why is it only used in the decoder?\n",
        "7. Why does the decoder use both self-attention and encoder-decoder attention?\n",
        "8. Why is the Transformer model parallelizable, and how does this improve efficiency compared to RNNs?\n",
        "9. How does multi-head self-attention allow the model to capture different aspects of a sentence?\n",
        "10. What does the decoder's final output represent before the projection layer? What does the encoder's final output represent?\n",
        "11. What is the role of the final linear projection layer in the decoder?\n",
        "How does the decoder output differ between training (parallel processing) and inference (sequential generation)?\n",
        "12. Why does the decoder recompute all outputs at each inference step instead of appending new outputs incrementally?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3tQdusIjPCy"
      },
      "source": [
        "# Small report - experiments of your own choice (15 points)\n",
        "Once everything is working fine, you can explore aspects of these models and do some research of your own into how they behave.\n",
        "\n",
        "For exemple, you can experiment with the hyperparameters.\n",
        "What are the effect of the differents hyperparameters with the final model performance? What about training time? If you decide to implement Greedy search to compare with beam search, how much worse is it ?\n",
        "\n",
        "What are some other metrics you could have for machine translation? Can you compute them and add them to your WandB report?\n",
        "\n",
        "Those are only examples, you can do whatever you think will be interesting.\n",
        "This part accounts for many points, *feel free to go wild!*\n",
        "\n",
        "---\n",
        "*Make a concise report about your experiments here.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8lJBVoElsQJw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U6-OpYG_hz2R"
      },
      "source": [
        "---\n",
        "# Not Part of TP3, But A Potential Project Idea: Understanding the Architecture of a Decoder-Only Transformer\n",
        "\n",
        "Step 1: In a project group of 3-4 create a high level plan for a Decoder-Only model for how you would need to modify this code to implement a Decoder-Only Transformer. Key components of the implementation should be split up and each member of the group should present the pseudo-code (or actual code) for one component of the full model to one another, and in a report. Then create the working model and perform experiments comparing it with your TP3 encoder-decoder model.\n",
        "\n",
        "For more details on the Decoder-Only Transformer see [this blog post](https://medium.com/international-school-of-ai-data-science/building-custom-gpt-with-pytorch-59e5ba8102d4). The [first \"GPT\" paper](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf), and the paper cited by this GPT-1 paper for the Decoder Only architecture used for GPT, [i.e. this paper](https://arxiv.org/abs/1801.10198)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxFnLwN1zI-h"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}