{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajouter src au path pour importer les modules\n",
    "import sys\n",
    "sys.path.append('src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from siglip.model import SiglipVisionModel\n",
    "from siglip.config import SiglipVisionConfig\n",
    "\n",
    "from paligemma.config import PaliGemmaConfig\n",
    "from paligemma.model import PaliGemmaForConditionalGeneration\n",
    "import torch\n",
    "import einops\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===========================================================================\n",
       "Layer (type:depth-idx)                             Param #\n",
       "===========================================================================\n",
       "SiglipVisionModel                                  --\n",
       "├─SiglipVisionTransformer: 1-1                     --\n",
       "│    └─SiglipVisionEmbeddings: 2-1                 --\n",
       "│    │    └─Conv2d: 3-1                            393,728\n",
       "│    │    └─Embedding: 3-2                         100,352\n",
       "│    └─SiglipEncoder: 2-2                          --\n",
       "│    │    └─ModuleList: 3-3                        --\n",
       "│    │    │    └─SiglipEncoderLayer: 4-1           --\n",
       "│    │    │    │    └─SiglipAttention: 5-1         1,050,624\n",
       "│    │    │    │    └─SiglipMLP: 5-2               2,099,712\n",
       "│    │    │    │    └─LayerNorm: 5-3               1,024\n",
       "│    │    │    │    └─LayerNorm: 5-4               1,024\n",
       "│    │    │    └─SiglipEncoderLayer: 4-2           --\n",
       "│    │    │    │    └─SiglipAttention: 5-5         1,050,624\n",
       "│    │    │    │    └─SiglipMLP: 5-6               2,099,712\n",
       "│    │    │    │    └─LayerNorm: 5-7               1,024\n",
       "│    │    │    │    └─LayerNorm: 5-8               1,024\n",
       "│    │    │    └─SiglipEncoderLayer: 4-3           --\n",
       "│    │    │    │    └─SiglipAttention: 5-9         1,050,624\n",
       "│    │    │    │    └─SiglipMLP: 5-10              2,099,712\n",
       "│    │    │    │    └─LayerNorm: 5-11              1,024\n",
       "│    │    │    │    └─LayerNorm: 5-12              1,024\n",
       "│    │    │    └─SiglipEncoderLayer: 4-4           --\n",
       "│    │    │    │    └─SiglipAttention: 5-13        1,050,624\n",
       "│    │    │    │    └─SiglipMLP: 5-14              2,099,712\n",
       "│    │    │    │    └─LayerNorm: 5-15              1,024\n",
       "│    │    │    │    └─LayerNorm: 5-16              1,024\n",
       "│    │    │    └─SiglipEncoderLayer: 4-5           --\n",
       "│    │    │    │    └─SiglipAttention: 5-17        1,050,624\n",
       "│    │    │    │    └─SiglipMLP: 5-18              2,099,712\n",
       "│    │    │    │    └─LayerNorm: 5-19              1,024\n",
       "│    │    │    │    └─LayerNorm: 5-20              1,024\n",
       "│    │    │    └─SiglipEncoderLayer: 4-6           --\n",
       "│    │    │    │    └─SiglipAttention: 5-21        1,050,624\n",
       "│    │    │    │    └─SiglipMLP: 5-22              2,099,712\n",
       "│    │    │    │    └─LayerNorm: 5-23              1,024\n",
       "│    │    │    │    └─LayerNorm: 5-24              1,024\n",
       "│    │    │    └─SiglipEncoderLayer: 4-7           --\n",
       "│    │    │    │    └─SiglipAttention: 5-25        1,050,624\n",
       "│    │    │    │    └─SiglipMLP: 5-26              2,099,712\n",
       "│    │    │    │    └─LayerNorm: 5-27              1,024\n",
       "│    │    │    │    └─LayerNorm: 5-28              1,024\n",
       "│    │    │    └─SiglipEncoderLayer: 4-8           --\n",
       "│    │    │    │    └─SiglipAttention: 5-29        1,050,624\n",
       "│    │    │    │    └─SiglipMLP: 5-30              2,099,712\n",
       "│    │    │    │    └─LayerNorm: 5-31              1,024\n",
       "│    │    │    │    └─LayerNorm: 5-32              1,024\n",
       "│    │    │    └─SiglipEncoderLayer: 4-9           --\n",
       "│    │    │    │    └─SiglipAttention: 5-33        1,050,624\n",
       "│    │    │    │    └─SiglipMLP: 5-34              2,099,712\n",
       "│    │    │    │    └─LayerNorm: 5-35              1,024\n",
       "│    │    │    │    └─LayerNorm: 5-36              1,024\n",
       "│    │    │    └─SiglipEncoderLayer: 4-10          --\n",
       "│    │    │    │    └─SiglipAttention: 5-37        1,050,624\n",
       "│    │    │    │    └─SiglipMLP: 5-38              2,099,712\n",
       "│    │    │    │    └─LayerNorm: 5-39              1,024\n",
       "│    │    │    │    └─LayerNorm: 5-40              1,024\n",
       "│    │    │    └─SiglipEncoderLayer: 4-11          --\n",
       "│    │    │    │    └─SiglipAttention: 5-41        1,050,624\n",
       "│    │    │    │    └─SiglipMLP: 5-42              2,099,712\n",
       "│    │    │    │    └─LayerNorm: 5-43              1,024\n",
       "│    │    │    │    └─LayerNorm: 5-44              1,024\n",
       "│    │    │    └─SiglipEncoderLayer: 4-12          --\n",
       "│    │    │    │    └─SiglipAttention: 5-45        1,050,624\n",
       "│    │    │    │    └─SiglipMLP: 5-46              2,099,712\n",
       "│    │    │    │    └─LayerNorm: 5-47              1,024\n",
       "│    │    │    │    └─LayerNorm: 5-48              1,024\n",
       "│    └─LayerNorm: 2-3                              1,024\n",
       "===========================================================================\n",
       "Total params: 38,323,712\n",
       "Trainable params: 38,323,712\n",
       "Non-trainable params: 0\n",
       "==========================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_siglip = SiglipVisionConfig(\n",
    "\tmodel_name=\"siglip-vision\",\n",
    "\thidden_size=512,\n",
    "\tintermediate_size=2048,\n",
    "\tnum_hidden_layers=12,\n",
    "\tnum_attention_heads=8,\n",
    ")\n",
    "\n",
    "model = SiglipVisionModel(config_siglip)\n",
    "# Print the model architecture\n",
    "summary(model, depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================\n",
       "Layer (type:depth-idx)                                       Param #\n",
       "=====================================================================================\n",
       "PaliGemmaForConditionalGeneration                            --\n",
       "├─SiglipVisionModel: 1-1                                     --\n",
       "│    └─SiglipVisionTransformer: 2-1                          --\n",
       "│    │    └─SiglipVisionEmbeddings: 3-1                      --\n",
       "│    │    │    └─Conv2d: 4-1                                 393,728\n",
       "│    │    │    └─Embedding: 4-2                              100,352\n",
       "│    │    └─SiglipEncoder: 3-2                               --\n",
       "│    │    │    └─ModuleList: 4-3                             --\n",
       "│    │    │    │    └─SiglipEncoderLayer: 5-1                3,152,384\n",
       "│    │    │    │    └─SiglipEncoderLayer: 5-2                3,152,384\n",
       "│    │    │    │    └─SiglipEncoderLayer: 5-3                3,152,384\n",
       "│    │    │    │    └─SiglipEncoderLayer: 5-4                3,152,384\n",
       "│    │    └─LayerNorm: 3-3                                   1,024\n",
       "├─PaliGemmaMultiModalProjector: 1-2                          --\n",
       "│    └─Linear: 2-2                                           1,050,624\n",
       "├─GemmaForCausalLM: 1-3                                      --\n",
       "│    └─GemmaModel: 2-3                                       --\n",
       "│    │    └─Embedding: 3-4                                   15,627,264\n",
       "│    │    └─ModuleList: 3-5                                  --\n",
       "│    │    │    └─GemmaDecoderLayer: 4-4                      --\n",
       "│    │    │    │    └─GemmaAttention: 5-5                    4,194,304\n",
       "│    │    │    │    └─GemmaMLP: 5-6                          3,145,728\n",
       "│    │    │    │    └─GemmaRMSNorm: 5-7                      512\n",
       "│    │    │    │    └─GemmaRMSNorm: 5-8                      512\n",
       "│    │    │    └─GemmaDecoderLayer: 4-5                      --\n",
       "│    │    │    │    └─GemmaAttention: 5-9                    4,194,304\n",
       "│    │    │    │    └─GemmaMLP: 5-10                         3,145,728\n",
       "│    │    │    │    └─GemmaRMSNorm: 5-11                     512\n",
       "│    │    │    │    └─GemmaRMSNorm: 5-12                     512\n",
       "│    │    │    └─GemmaDecoderLayer: 4-6                      --\n",
       "│    │    │    │    └─GemmaAttention: 5-13                   4,194,304\n",
       "│    │    │    │    └─GemmaMLP: 5-14                         3,145,728\n",
       "│    │    │    │    └─GemmaRMSNorm: 5-15                     512\n",
       "│    │    │    │    └─GemmaRMSNorm: 5-16                     512\n",
       "│    │    │    └─GemmaDecoderLayer: 4-7                      --\n",
       "│    │    │    │    └─GemmaAttention: 5-17                   4,194,304\n",
       "│    │    │    │    └─GemmaMLP: 5-18                         3,145,728\n",
       "│    │    │    │    └─GemmaRMSNorm: 5-19                     512\n",
       "│    │    │    │    └─GemmaRMSNorm: 5-20                     512\n",
       "│    │    └─GemmaRMSNorm: 3-6                                512\n",
       "│    └─Linear: 2-4                                           15,627,264\n",
       "=====================================================================================\n",
       "Total params: 74,774,528\n",
       "Trainable params: 74,774,528\n",
       "Non-trainable params: 0\n",
       "====================================================================================="
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dict\n",
    "vision_config = {\n",
    "\t\"hidden_size\": 512,\n",
    "\t\"intermediate_size\": 2048,\n",
    "\t\"num_hidden_layers\": 4,\n",
    "\t\"num_attention_heads\": 8,\n",
    "\t\"layer_norm_eps\": 1e-12,\n",
    "\t\"max_position_embeddings\": 512,\n",
    "\t\"vocab_size\": 30522,\n",
    "\t\"image_size\": 224,\n",
    "\t\"patch_size\": 16,\n",
    "}\n",
    "text_config = {\n",
    "\t\"hidden_size\": 512,\n",
    "\t\"intermediate_size\": 2048,\n",
    "\t\"num_hidden_layers\": 4,\n",
    "\t\"num_attention_heads\": 8,\n",
    "\t\"layer_norm_eps\": 1e-12,\n",
    "\t\"max_position_embeddings\": 512,\n",
    "\t\"vocab_size\": 30522,\n",
    "\t\"num_key_value_heads\": 8,\n",
    "}\n",
    "config_pali = PaliGemmaConfig(\n",
    "\tmodel_name=\"paligemma\",\n",
    "\thidden_size=512,\n",
    "\tintermediate_size=2048,\n",
    "\tnum_hidden_layers=4,\n",
    "\tnum_attention_heads=8,\n",
    "\tvision_config=vision_config,\n",
    "\ttext_config=text_config,\n",
    ")\n",
    "model = PaliGemmaForConditionalGeneration(config_pali)\n",
    "# Print the model architecture\n",
    "summary(model, depth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 512, 49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 49, 512])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test\n",
    "B, E, H, W = 2, 512, 7, 7\n",
    "patch_embeds = torch.randn(B, E, H, W)\n",
    "# [Batch_Size, Embed_Dim, Num_Patches_H, Num_Patches_W] -> [Batch_Size, Embed_Dim, Num_Patches]\n",
    "# where Num_Patches = Num_Patches_H * Num_Patches_W\n",
    "embeddings = patch_embeds.flatten(2)\n",
    "# [Batch_Size, Embed_Dim, Num_Patches] -> [Batch_Size, Num_Patches, Embed_Dim]\n",
    "embeddings = embeddings.transpose(1, 2)\n",
    "\n",
    "embeddings.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_2 = einops.rearrange(patch_embeds, \"b e h w -> b (h w) e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(embeddings == embeddings_2).all() # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "from einops import repeat\n",
    "from time import time\n",
    "# Comparaison ds 2 fonctions : \n",
    "def __repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "\tbatch_size, num_key_value_heads, seq_len, head_dim = hidden_states.size()\n",
    "\t# On répète les clés et valeurs pour chaque tête de query\n",
    "\tif n_rep == 1:\n",
    "\t\treturn hidden_states\n",
    "\t\n",
    "\thidden_states = hidden_states[:, :, None, :, :].expand(batch_size, num_key_value_heads, n_rep, seq_len, head_dim)\n",
    "\treturn hidden_states.reshape(batch_size, num_key_value_heads * n_rep, seq_len, head_dim)\n",
    "\n",
    "def _repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n",
    "\t\"\"\"\n",
    "\tRépète les clés et valeurs pour chaque tête de query.\n",
    "\tArgs:\n",
    "\t\thidden_states (torch.Tensor): Les clés ou valeurs. [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
    "\t\tn_rep (int): Le nombre de répétitions.\n",
    "\tReturns:\n",
    "\t\ttorch.Tensor: Les clés ou valeurs répétées. [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
    "\t\"\"\"\n",
    "\tif n_rep == 1:\n",
    "\t\treturn hidden_states\n",
    "\t\n",
    "\treturn repeat(hidden_states, 'b h s d -> b (h r) s d', r=n_rep)\n",
    "\n",
    "\n",
    "# Test\n",
    "B, H, S, D = 2, 8, 7, 64\n",
    "hidden_states = torch.randn(B, H, S, D)\n",
    "n_rep = 4\n",
    "# Test de la fonction __repeat_kv\n",
    "repeated_states = __repeat_kv(hidden_states, n_rep)\n",
    "# Test de la fonction _repeat_kv\n",
    "repeated_states_2 = _repeat_kv(hidden_states, n_rep)\n",
    "\n",
    "# Vérification de l'égalité\n",
    "print((repeated_states == repeated_states_2).all())  # True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__repeat_kv : 6.073018312454224 sec\n",
      "repeat_kv avec einops : 4.460999011993408 sec\n"
     ]
    }
   ],
   "source": [
    "# Test de la vitesse\n",
    "start = time()\n",
    "n_time = 100_000\n",
    "for i in range(n_time):\n",
    "\t__repeat_kv(hidden_states, n_rep)\n",
    "end = time()\n",
    "print(f\"__repeat_kv : {end - start} sec\")\n",
    "start = time()\n",
    "for i in range(n_time):\n",
    "\t_repeat_kv(hidden_states, n_rep)\n",
    "end = time()\n",
    "print(f\"repeat_kv avec einops : {end - start} sec\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ 1,  2,  3,  4],\n",
      "          [ 5,  6,  7,  8]],\n",
      "\n",
      "         [[ 9, 10, 11, 12],\n",
      "          [13, 14, 15, 16]]]])\n",
      "x1,  tensor([[[[ 1,  2],\n",
      "          [ 5,  6]],\n",
      "\n",
      "         [[ 9, 10],\n",
      "          [13, 14]]]])\n",
      "x2,  tensor([[[[ 3,  4],\n",
      "          [ 7,  8]],\n",
      "\n",
      "         [[11, 12],\n",
      "          [15, 16]]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[[ -3,  -4,   1,   2],\n",
       "          [ -7,  -8,   5,   6]],\n",
       "\n",
       "         [[-11, -12,   9,  10],\n",
       "          [-15, -16,  13,  14]]]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test de rotate\n",
    "def rotate_half(x: torch.Tensor) -> torch.Tensor:\n",
    "\t\"\"\"\n",
    "\tEffectue une rotation de moitié sur les clés et valeurs.\n",
    "\tPermet de construire le vecteur [-x2, x1, -x4, x3, ...]\n",
    "\tArgs:\n",
    "\t\tx (torch.Tensor): Les clés ou valeurs. [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
    "\tReturns:\n",
    "\t\ttorch.Tensor: Les clés ou valeurs après rotation. [Batch_Size, Num_Heads_KV, Seq_Len, Head_Dim]\n",
    "\t\"\"\"\n",
    "\tx1 = x[..., :x.size(-1) // 2] # Récupère la première moitié\n",
    "\tprint(\"x1, \", x1)\n",
    "\tx2 = x[..., x.size(-1) // 2:] # Récupère la deuxième moitié\n",
    "\tprint(\"x2, \", x2)\n",
    "\treturn torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "# Test\n",
    "hidden_states = torch.tensor([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]).reshape(1, 2, 2, 4) # [Batch_Size, Num_Heads_KV, Head_Dim]\n",
    "print(hidden_states)\n",
    "# Test de la fonction rotate_half\n",
    "rotated_states = rotate_half(hidden_states)\n",
    "rotated_states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(True)\n",
      "tensor(True)\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# test :\n",
    "# freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2)\n",
    "# Autre\n",
    "# freqs = torch.einsum('bdr,brs->bsd', freqs, position_ids_expanded)\n",
    "\n",
    "# Test\n",
    "# [Batch_Size, Head_Dim // 2, 1]\n",
    "inv_freq_expanded = torch.randn(3, 4, 1)\n",
    "\n",
    "# [Batch_Size, 1, Seq_Len]\n",
    "position_ids_expanded = torch.randn(3, 1, 5)\n",
    "\n",
    "# [Batch_Size, Head_Dim // 2, Seq_Len]\n",
    "freqs = (inv_freq_expanded @ position_ids_expanded).transpose(1, 2)\n",
    "\n",
    "\n",
    "freqs_2 = torch.einsum('bdr,brs->bsd', inv_freq_expanded, position_ids_expanded)\n",
    "freqs_3 = torch.einsum('bdr,brs->bds', inv_freq_expanded, position_ids_expanded).transpose(1, 2)\n",
    "\n",
    "print((freqs == freqs_2).all())  # True\n",
    "print((freqs == freqs_3).all())  # True\n",
    "print((freqs_2 == freqs_3).all())  # True"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
